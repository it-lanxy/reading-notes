
#第1章 Linux内核简介
##1.1 Unix的历史
    Unix是一个设计简洁，支持抢占式多任务、多线程、虚拟内存、换页、动态链接和TCP/IP网络的现代化操作系统。
    Unix强大的根本原因：
       1，Unix很简洁
       2，所有的东西都被当做文件对待。这种抽象使对数据和对设备的操作是通过一套相同的系统调用接口来进行的：open()/read()/write()/lseek()和close()。
       3，内核和相关的系统软件是用c语言编写的，移植能力强。
       4，Unix进程创建非常迅速，并且有一个非常独特的fork()系统调用。
       5，Unix提供了一套非常简单但又很稳定的进程间通信元语，快速简洁的进程创建过程使Unix的程序把目标放在一次执行保质保量地完成一个任务上，而简单稳定的进程间通信机制又可以保证这些单一目的的简单程序可以方便地组合在一起，去解决现实中变得越来越复杂的任务。
       正是由于这种策略和机制分离的设计理念，确保了Unix系统具备了清晰的层次化结构。
##1.2 追寻Linus足迹：Linux简介
    Linux是类Unix系统，但它不是Unix，开源
##1.3 操作系统和内核简介
    操作系统是指在整个系统中负责完成最基本功能和系统管理的那些部分。这些部分应该包括内核、设备驱动程序、启动引导程序、命令行shell或者其他种类的用户界面、基本的文件管理工具和系统工具。
    用户界面是操作系统的外在表象，内核才是操作系统的内在核心。
    通常一个内核由负责响应中断的中断服务程序，负责管理多个进程从而分享处理器时间的调度程序，负责管理进程地址空间的内存管理程序和网络、进程间通信等系统服务程序共同组成。对于提供保护机制的现代操作系统来说，内核独立于普通应用程序，它一般处于系统态，拥有受保护的内存空间和访问硬件设备的所有权限。这种系统态和被保护起来的内存空间，统称为内核空间。
    相对的，应用程序在用户空间执行。它们只能看到允许它们使用的部分系统资源，而且只能使用某些特定的系统功能，不能直接访问硬件，也不能访问内核划给别人的内存范围，还有一些其他使用限制。
    当内核运行的时候，系统以内核态进入内核空间执行。
    而执行一个普通用户程序时，系统将以用户态进入以用户空间执行。
    
    系统中运行的应用程序通过系统调用与内核通信。应用程序通常调用库函数（比如C库函数）再由库函数通过系统调用界面，让内核代其完成各种不同的任务。
    当一个应用程序执行一条系统调用，我们说内核正在代其执行。如果进一步解释，在这种情况下，应用程序被称为通过系统调用在`内核空间运行`，而内核被称为运行于`进程上下文中`。这种交互关系————应用通过系统调用界面陷入内核————是应用程序完成其工作的基本行为方式。
    
    内核还要负责管理系统的硬件设备。当硬件设备想和系统通信的时候，它首先要发出一个异步的`中断信号`（中断机制）去打断处理器的执行，继而打断内核的执行。中断通常对应着一个中断号，内核通过这个中断号查找相应的中断服务程序，并调用这个程序响应和处理中断。
    举个例子，当你敲击键盘的时候，键盘控制器发送一个中断信号告知系统，键盘缓冲区有数据到来。内核注意到这个中断对应的中断号，调用相应的中断服务程序。该服务程序处理键盘数据然后通知键盘控制器可以继续输入数据了。
    
    许多操作系统的中断服务程序（通过中断号查找），都不在进程的上下文中执行（中断上下文）。它们在一个与所有进程都无关的、专门的中断上下文中运行。之所以存在这样一个专门的执行环境，就是为了保证中断服务程序能够在第一时间响应和处理中断请求，然后快速的退出。
    
    上下文代表着内核活动的范围：
        运行于用户空间，执行用户进程。
        运行于内核空间，处于进程上下文，代表某个特定的进程执行。
        运行于内核空间，处于中断上下文，与任何进程无关，处理某个特定的中断。
    当CPU空闲时，内核就运行一个空进程，处于进程上下文，但运行于内核空间。
##1.4 Linux内核和传统Unix内核的比较
    Unix内核通常需要硬件系统提供页机制（MMU）以管理内存。这种页机制可以加强对内核空间的保护，并保证每个进程都可以运行于不同的虚地址空间上。
###单内核与微内核设计之比较
    操作系统内核可以分为两大阵营：单内核和微内核（第三阵营是外内核，主要用在科研系统上）。
    所谓单内核（单模块内核）就是把它从整体上作为一个单独的大过程来实现，同时也运行在一个单独的地址空间上（理论上有的处于内核空间，有的处于用户空间）。因此，这种内核通常以单个静态二进制文件的形式存放于磁盘中。所有内核服务都在这样的一个大内核地址空间上运行。内核之间的通信是微不足道的，因为大家都运行在内核态，并伸出同一地址空间：内核可直接调用函数，这与用户空间应用程序无区别。大多数Unix系统都设计为单模块，单模块具有简单和性能高的特点。
    微内核的功能被划分为多个独立的过程，每个过程叫做一个服务器。所有服务器都保持独立并运行在各自的地址空间上。因此，不能像单内核那样直接调用函数，而是通过消息传递处理微内核通信：系统采用了进程间通信（IPC)机制。通过IPC机制互通消息，互换`服务`。服务器的各自独立有效地避免了一个服务器失效祸及另一个。同样，模块化的系统允许一个服务器为了另一个服务器而换出。
    比较：
        因为IPC机制的开销多于系统调用，又因为会涉及内核空间与用户空间的上下文切换，因此，消息传递需要一定的周期，而单内核中简单的函数调用没有这些开销。结果，为减少开销，所有实际应用的基于微内核的系统都让大部分或全部服务器位于内核，这样，就可以直接调用函数，消除频繁的上下文切换。
    Linux是一个汲取了微内核精华的单内核，运行于单独的内核地址空间上，并借鉴微内核的模块化设计、抢占式内核、支持内核线程以及动态装载内核模块的能力。Linux让所有事情都运行在内核态（微内核要求有强权），直接调用函数，无消息传递。      
    
    Linux和Unix系统之间的差异：
        Linux支持动态加载内核模块。
        Linux支持对称处理（SMP）机制（SMP (Symmetric Multi Processing),对称多处理系统内有许多紧耦合多处理器，在这样的系统中，所有的CPU共享全部资源，如总线，内存和I/O系统等，操作系统或管理数据库的复本只有一个，这种系统有一个最大的特点就是共享所有资源。多个CPU之间没有区别，平等地访问内存、外设、一个操作系统。操作系统管理着一个队列，每个处理器依次处理队列中的进程。如果两个处理器同时请求访问一个资源（例如同一段内存地址），由硬件、软件的锁机制去解决资源争用问题。）。
        Linux内核可以抢占。Linux内核具有允许在内核运行的任务有限执行的能力。
        Linux对线程支持的实现比较有意思：内核并不区分线程和其他的一般进程。对于内核来说，所有的进程都一样————只不过是其中的一些共享资源而已。
        Linux提供具有设备类的面向对象的设备模型、热插拔时间，以及用户空间的设备文件系统（sysfs）
#第2章 从内核触发
    git clone git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
#第3章 进程管理
    如何管理每个进程：它们在内核中如何被列举，如何创建，最终如何消亡。操作系统就是为了运行用户程序，因此，进程管理就是所有操作系统的心脏所在。
##3.1 进程
    进程就是处于执行期的程序（目标码存放在某种介质上）。但进程并不仅仅局限于一段可执行程序代码。通常进程还包含其他资源，像打开的文件，挂起的信号，内核内部数据，处理器状态，一个或多个具有内存映射的内存地址空间及一个或多个执行线程，用来存放全局变量的数据段等。
    执行线程，简称线程（thread），是在进程中活动的对象。每个线程都拥有一个独立的程序计数器、进程栈和一组进程寄存器。内核调度的对象是线程，而不是进程。对Linux而言，线程只不过是一种特殊的进程罢了。
    在现代操作系统中，进程提供两种虚拟机制：虚拟处理器和虚拟内存（通过虚拟内存来感受虚拟处理器就是不是真实的处理器）。实际上许多进程正在分享一个处理器，但虚拟处理器让这些进程觉得自己独享处理器（此为虚拟处理器，虚在`独享`的错觉）。而虚拟内存让进程在分配和管理内存时觉得自己拥有整个系统的所有内存资源（有这个必要么？）。有趣的是，线程之间可以共享虚拟内存，但是每个都拥有各自的虚拟处理器。
    进程在创建它的时候存活。linux系统中，这通常是调用fork()（fork实际上是由clone()系统调用实现的）系统的结果（fork 创建进程），该系统调用通过复制一个现有的进程来创建一个全新的进程。父进程调用fork()操作创建子进程。在fork()调用结束时，在返回点这个相同位置上，父进程恢复执行，子进程开始执行。fork()系统调用从内核返回两次：一次回到父进程，另一次回到新产生的子进程。紧接着调用exec()这组函数就可以创建新的地址空间，并把新的程序载入其中。最终调用exit()系统调用退出执行。终结进程并释放资源。父进程可以通过wait4()查询子进程是否终结（父进程拥有等待特定进程执行完毕的能力）。进程退出后被设置为僵死状态，直到父进程调用wait()或waitpit()位置。
####进程的另一个名字是任务（task）。本书中的任务通常指的是从内核观点看到的进程。
##3.2 进程描述符及任务结构
    内核把进程的列表存放在叫做任务队列（task list）的双向循环链表中。链表中的每一个项都是类型为task_struct（知道为啥进程也叫任务了吧）、称为进程描述符（process descriptor）的结构，里面有pid。结构体定义在：linux/include/linux/sched.h:637。进程描述符中包含一个具体进程的所有信息。
    task_struct在32位机器上，大约有1.7KB。包含了内核管理一个进程所需的所有信息，包括：它打开的文件，进程的地址空间，挂起的信号，进程的状态等。
###3.2.1 分配进程描述符
    Linux通过slab分配器分配task_struct结构，这样能达到对象复用（Unix的一个特点是创建进程迅速：复用可避免动态分配和释放所带来的资源消耗）和缓存着色（cache coloring）的目的。slab分配器动态生成task_struct，只需在栈底或栈顶创建一个新的结构struct thread_info，thread_info有一个指向进程描述符（task_struct）的指针。
###3.2.2 进程描述符的存放
    内核通过一个唯一的进程标识符（process identification valud）或PID来标识每个进程。PID是一个int类型的数，最大值默认设置为32768（short int短整形的最大值）可通过linux/include/linux/threads.h:28 修改，这个最大值是系统中允许同时存在的进程最大数目，修改/proc/sys/kernel/pid_max来提高上限。PID存放在task_struct进程描述符里。
###3.2.3 进程状态
    进程描述符task_struct中的state域描述了进程的当前状态。系统中的每个进程都必然处于五种进程状态中的一种。
        TASK_RUNNING（运行）————进程可执行的：它或者正在执行，或者在运行队列中等待执行。进程在用户空间中执行的唯一状态。
        TASK_INTERRUPTIBLE（可中断）————进程正在休眠（也就是说它被阻塞），等待某些条件的达成。一旦这些条件达成，内核就会把进程状态设置为运行。处于此状态的进程也会因为接收到信号而提前被唤醒并随时准备投入运行（Thread.sleep可被提前唤醒吗？）。
        TASK_UNINTERRUPTIBLE（不可中断）————就算是接收到信号也不会被唤醒或准备投入运行外，这个状态与可打断状态相同（进程正在休眠、阻塞）。
        _TASK_TRACED————被其他进程跟踪的进程，例如通过ptrace对调试程序进行跟踪
        _TASK_STOPPED（停止）————进程停止执行
###3.2.4 设置当前进程状态
    内核经常需要调整某个进程的状态。这时最好使用set_task_state(task，state)函数
    该函数将指定的进程设置为指定的状态。必要的时候，它会设置内存屏障来强制其他处理器作重新排序。
###3.2.5 进程上下文
    可执行程序时进程的重要组成部分。这些代码从一个可执行文件载入到进程的地址空间执行。一般程序在用户地址空间执行。当一个程序执行了系统调用或者出发了某个异常，它就陷入了内核空间，内核处于进程上下文，代表用户进程执行（系统调用）。除非在此间隙有更高优先级的进程需要执行并由调度器做出了相应调整，否则在内核退出的时候，程序恢复在用户空间会继续执行。
###3.2.6 进程家族树
    所有进程都是PID为1的进程的后代。每个进程必有一个父进程，同一个父进程的称为兄弟。
##3.3 进程的创建
    别的操作系统都会提供产生（spawn）进程的机制，首先在新的地址空间里创建进程，读入可执行文件，最后开始执行。
    Unix把上述步骤分解到两个单独的函数中去执行：fork()和exec()。首先fork()通过拷贝当前进程创建一个子进程。子进程与父进程的区别仅仅在于PID、PPID（父进程进程号）和某些资源和统计量（例如，挂起的信号，它没必要集成）。exec()函数负责读取可执行文件并将其载入地址空间开始运行。
###3.3.1 写时拷贝
    传统的fork()系统调用直接把所有的资源复制给新创建的进程。这种实现过于简单并且效率低下，因为它拷贝的数据也许并不共享，更糟的情况是，如果新进程打算立即执行一个新的映像，那么拷贝没有意义。Linux的fork()使用写时拷贝(copy-on-write)页实现。写时拷贝是一种可以推迟（只有需要写入的时候，数据才会被复制，从而使各个进程拥有各自的拷贝）甚至免除拷贝数据（只需要只读，不涉及写，甚至俩进程一点关系都没有，不像redis fork 父子进程）的技术。内核此时并不复制整个进程地址空间，而是让父进程和子进程共享同一个拷贝。
    fork()的实际开销就是复制父进程的页表以及给子进程创建唯一的进程描述符。
    写时拷贝技术可以避免拷贝大量根本就不会被使用的数据（地址空间里常常包含数十兆的数据）。由于Unix强调进程快速执行的能力，所以这个优化很重要。
###3.3.2 fork()
    Linux通过clone()系统调用实现fork()。这个调用通过一系列的参数标志来指明父、子进程需要共享的资源。过程详询p27。
###3.3.3 vfork()
    除了不拷贝父进程的页表项外，vfork()系统调用和fork()的功能相同。子进程作为父进程的一个单独的线程在它的地址空间里运行，父进程被阻塞，直到子进程退出或执行exec()。子进程不能向地址空间写入。过程详询p27。
##3.4 线程在Linux中的实现
    线程机制是现代编程技术中常用的一种抽象概念。该机制提供了在同一程序内共享内存地址空间运行的一组线程。这些线程还可以共享打开的文件和其他资源。线程机制支持并发程序设计技术，在多处理器系统上，它也能保证真正的并行处理。
    Linux实现线程的机制非常独特。从内核的角度来说，它并没有线程这个概念。Linux把所有的线程都当做进程来实现。内核并没有特别的调度算法或是定义特别的数据结构来表征线程。相反，线程仅仅被视为一个与其他进程共享某些资源的进程。每个线程都拥有唯一隶属于自己的task_struct（进程描述符），所以在内核中，它看起来就像是一个普通的进程（只是线程和其他一些进程共享某些资源，如地址空间）。
    在Windows等操作系统，在内核中提供了专门支持线程的机制（这些系统常常把线程称作轻量级进程）。与Linux有差异。在其他系统中，相较于重量级进程，线程被抽象成一种消耗较少资源，运行迅速的执行单元。而对于Linux来说，它只是一种进程间共享资源的手段（Linux进程本身够轻量了）。
    举个例子，加入我们有一个包含四个线程的进程，在提供专门线程支持的系统中，通常会有一个包含指向四个不同线程的指针的进程描述符。该描述符负责描述像地址空间、打开的文件这样的共享资源。线程本身再去描述它独占的资源。相反，Linux仅仅创建四个进程并分配四个普通的task_struct结构。建立这四个进程时指定他们共享某些资源，这是相当高雅的做法。
###3.4.1 创建线程
    线程的创建和普通进程的创建类似，只不过在调用clone()的时候需要传递一些参数标志来指明需要共享的资源。
###3.4.2 内核线程
    内核经常需要在后台执行一些操作。这种任务可以通过内核线程（kernel thread）完成————独立运行在内核空间的标准进程。内核线程和普通的进程间的区别在于内核线程没有独立的地址空间。只在内核空间运行，从来不切换到用户空间去。内核进程和普通进程一样，可以被调度和抢占。
##3.5 进程终结
    当一个进程终结时，内核必须释放它所占有的资源并把这以不幸告知其父进程。父进程检查后，进程所持有的所有内存都会被系统回收。
###3.5.1 删除进程描述符
    在调用do_exit()后，尽管线程已经僵死不能再运行了，但是系统还保留了它的进程描述符。系统此时保留进程描述符是可以上系统有办法在子进程终结后仍能获得它的信息。因此，进程终结时所需的清理工作和进程描述符的删除被分开执行。在父进程获得已终结的子进程的信息后，或者通知内核它不关注那些信息后，子进程的task_struct会被释放。
###3.5.2 孤儿进程造成的进退维谷
    如果父进程在子进程之前退出，必须有机制来保证子进程能找到一个新的父亲，否则这些称谓孤儿的进程就会退出时永远的处于僵死状态（进程描述符未删除），白白地消耗内存。对于这个问题，解决办法是给子进程在当前线程组内找一个线程作为父亲，如果不行，就让init做它们的父进程。
##3.6 小结
    考察了操作系统的核心概念————进程。讨论了进程的一般特性，为什么如此重要，以及进程与线程的关系。然后讨论了Linux如何存放和表示进程（task_struct 和thread_info）,如何创建进程（通过fork(),实际上最终是clone()），如何把新的执行映像装入到地址空间（通过exec()系统调用簇），以及进程最终如何消亡（强制或自愿地调用exit())。
    
#第4章 进程调度
    第3章讨论了进程，它在操作系统看来是程序的运行态表现形式。本章将讨论进程调度程序，它是确保进程能有效工作的一个内核子系统。
##4.1 多任务
    多任务操作系统就是能同时并发地交互执行多个进程的操作系统。
    多任务操作系统都能使多个进程处于阻塞或睡眠状态（实际上不被投入执行，直到工作确实就绪）。这些任务尽管位于内存，但并不处于可运行状态。
    多任务系统可以划分为两类：非抢占式多任务和抢占式多任务。
    抢占式多任务模式：由调度程序决定什么时候停止一个进程的运行，以便其它进程能够得到执行机会。这个强制的挂起动作就叫做抢占。进程在被抢占之前能够运行的时间是预先分配好的，叫进程的时间片（timeslice）。时间片实际上就是分配给每个可运行进程的处理器时间段。有效管理时间片可避免个别进程独占系统资源。当今众多现代操作系统对程序运行都采用了动态时间片计算的方式，并且引入了可配置的计算策略。但Linux独一无二的`公平`调度程序本身并`没有采取时间片`来达到公平调度。
    非抢占式多任务模式：除非进程自己主动停止运行，否则它会一直执行。进程主动挂起自己的操作称为让步（yielding）。这种机制的缺点：
        （内核的）调度程序无法对每个进程该执行多长时间做出统一规定，所以进程独占的处理器时间可能超出用户的预料；
        更糟糕的是，一个绝不做出让步的悬挂进程就能使系统崩溃。
    绝大多数操作系统都采用了抢占式多任务。
##4.2 Linux的进程调度
    O(1)调度算法涉及：静态时间片算法和针对每一处理器的运行队列。
    （使用）O(1)的调度程序虽然对大服务器的工作负载很理想，但是在有很多交互程序要运行的桌面系统上表现不佳，因为其缺少交互进程。
    2.6版本内核引入一些调度算法，其中最著名的"反转楼梯最后期限调度算法"（RSDL），从而O(1)算法并更名为"完全公平调度算法"，简称CFS。
##4.3 策略
    策略决定调度程序在何时让什么进程运行。调度器的策略往往就决定系统的整体印象，并且，还要负责优化使用处理器时间。
###4.3.1 I/O消耗型和处理器消耗型的进程
    进程可以被分为I/O消耗型和处理器消耗型。前者指进程的大部分时间用来提交I/O请求或是等待I/O请求。因此，这样的进程经常处于可运行状态，但通常都是运行短短的一会儿，因为它在等待更多的I/O请求时最后总会阻塞（这里所说的I/O是指在任何类型的可阻塞资源，比如键盘输入，或者是网络I/O）。举例来说，多数用户图形界面程序（GUI）都属于I/O密集型，即使它们从不读取或者写入磁盘，它们也会在多数时间里都在等待来自鼠标或者键盘的用户交互操作。
    相反，处理器耗费型进程把时间大多用在执行代码上。除非被抢占，否则它们通常都一直不停地运行，因为它们没有太多的I/O需求。但是，因为它们不属于I/O驱动类型，所以从系统响应速度考虑，调度器不应该经常让他们运行。对于这类处理器消耗型的进程，调度策略往往是尽量降低它们的调度频率，而延长其运行时间。处理器消耗型进程的极端例子就是无限循环地执行。
    但是这两种划分方法并非是绝对的。
    调度策略通常要在两个矛盾的目标中寻找平衡：进程响应寻思（响应时间短）和最大系统利用率（高吞吐量）。所以，调度程序算法复杂，并且不保证低优先级进程被公平对待。Unix系统调度程序倾向于I/O消耗型程序，以提供更好的程序响应速度。Linux为了保证交互式应用和桌面系统性能，也倾向于优先调度I/O消耗型进程。虽然如此，下面你会看到，处理器并未忽略处理器消耗型的进程。
###4.3.2 进程优先级
    调度算法中最基本的一类就是基于优先级的调度。这是一种根据进程的价值和其对处理器时间的需求来对进程分级的想法。通常的做法是（并未被Linux系统完全采用）优先级高的进程先运行，低的后运行，相同优先级的进程按轮转方式进行调度（一个接一个，重复进行）。在某些系统中，优先级高的进程使用的时间片也较长。调度程序总是选择时间片未用尽而且优先级最高的进程运行。用户和系统都可以通过设置进程的优先级来影响系统的调度。
    Linux采用两种不同的优先级范围：
        第一种是nice值，范围-20~19，默认为0：越大的nice值意味着更低的优先级。相比高nice值（低优先级）的进程，低nice值（高优先级）的进程可以获得更多的处理器时间。可通过ps -el命令查看进程列表，NI就是nice值
        第二种范围是实时优先级，其值是可配置的，默认情况下它的变化范围是从0~99。与nice值意义想法，越高的实时优先级意味着进程优先级越高。可通过ps -eo state,uid,pid,ppid,rtprio,time,comm 查看。实时优先级在 RIPRIO列表下，其中如果有 -，则说明它不是实时进程。
###4.3.3 时间片
    时间片是一个数值，它表明进程在被抢占前所能持续运行的时间。调度策略必须规定一个默认的时间片。但是：时间片太长会导致系统对交互的响应表现欠佳，让人觉得系统无法并发执行应用程序；太短会明显增大进程切换带来的处理器耗时，因为肯定会有相当一部分系统时间用在进程切换上，而这些进程能够用来运行的时间片却很短。此外，I/O消耗型和处理器消耗型的进程之间的矛盾也在这里再次显露出来：I/O消耗型不需要长的时间片，而处理器消耗型的进程则希望越长越好（比如这样可以让它们的高速缓存命中率更高）。
    从上面的争论中可以看出，任何长时间片都将导致系统交互表现欠佳。很多系统中都特别重视这一点，所以默认的时间片很短，如10ms。但是Linux的CFS调度器并没有直接分配时间片到进程，它是将处理器的使用比划分给了进程。这样一来，进程所获得的处理器时间其实是和系统负载密切相关的。这个比例进一步还会受nice值，nice值作为权重将调整进程所使用的处理器时间使用比。
    Linux系统是抢占式的。当一个进程进入可运行态，它就被准许投入运行。在多数操作系统中，是否将一个进程立刻投入运行（也就是抢占当前进程），是完全由进程优先级和是否有时间片决定的。而在Linux中使用新的CFS调度器，其抢占时机取决于新的可运行程序消耗了多少处理器使用比。如果消耗的使用比比当前进程小，则新进程立刻投入运行，抢占当前进程。否则，将推迟其运行。
###4.3.4 调度策略的活动
    看书看书，太形象又没必要笔记
##4.4 Linux调度算法
    前面抽象讨论了进程调度原理，现在进一步探讨具有Linux特色的进程调度程序。      
###4.4.1 调度器类
    Linux调度器是以模块方式提供的，这样做的目的是允许不同类型的进程可以针对性地选择调度算法。
    这种模块化结构被称为调度器类（scheduler classes），它允许多种不同的可动态添加的调度算法并存，调度属于自己范畴的进程。
    完全公平调度（CFS）是一个针对普通进程的调度器类。
###4.4.2 Unix系统中的进程调度
    现代进程调度器有两个通用的概念：进程优先级和时间片。时间片是指进程运行了多少时间，进程一旦启动就会有一个默认时间片。具有更高优先级的进程将运行得更频繁，而且也会被赋予更多的时间片。在Unix系统上，优先级以nice值形式输出给用户空间。
    有三个问题：看书
###4.4.3 公平调度
    CFS的出发点基于一个简单的理念：进程调度的`效果`应如同`系统具备一个理想中的完美多任务处理器`。在这种系统中，每个进程将能获得1/n的处理器时间——n是指可运行进程的数量。同时，我们可以调度给它们无限小的时间周期，所以在任何可测量周期内，我们给予n个进程中每个进程同样多的运行时间。完美的多任务处理器模型时这样的（以两个进程为例）：我们能在10ms内同时运行两个进程，它们各自使用处理器一半的能力。
    理想模型并非线上（无法在一个处理器上真的同时运行多个进程）。而且如果每个进程运行无限小的时间周期也是不高效的————因为调度时进程抢占会带来一定的代价：将进程换出，另一个换入本身有消耗，同时还会影响到缓存的效率。
    CFS的做法是允许每个进程运行一段时间、循环轮转、选择运行最少的进程作为下一个运行的进程，而不再采用分配给每个进程时间片的做法了，CFS在所有可运行进程总数基础上计算出一个进程应该运行多久，而不是依靠nice值来计算时间片。
    当可运行任务数量趋于无限时，它们各自所获得的处理器使用比和时间片都趋于0.这样无疑造成了不可接受的切换消耗。CFS是怎么做的呢？看书
    总结一下，任何进程所获得的的处理器时间是由它自己和其他所有可运行进程nice值的相对差值决定的。nice值对时间片的作用不再是算术加权，而是几何加权。任何nice值对应的绝对时间不再是一个绝对值，而是处理器的使用比。CFS称为公平调度器是因为它确保给每个进程公平的处理器使用比。
##4.5 Linux调度的实现
    在讨论了CFS调度算法的动机和其内部逻辑后，我们现在可以开始具体探索CFS是如何得以实现的。我们将关注四个组成部分：
        时间记账、进程选择、调度器入口、睡眠和唤醒
###4.5.1 时间记账
    所有的调度器都必须对进程运行时间做记账。多数Unix系统，正如我们前面所说，分配一个时间片给每一个进程。那么当每次系统时钟节拍发生时，时间片都会被减少一个节拍周期。当一个进程的时间片被减少到0时，它就会被另一个尚未减到0的时间片可运行进程抢占。
    1.调度器实体结构
    CFS不再有时间片的概念，但是它也必须维护每个进程运行的时间记账，因为它需要确保每个进程只在公平分配给它的处理器时间内运行。  
    2.虚拟实时
    vruntime变量存放进程的虚拟运行时间（单位：ns），该运行时间的计算是经过了所有课运行进程总数的标准化。CFS使用vruntime变量来记录一个程序到底运行了多长时间以及它还应该再运行多久。
###4.5.2 进程选择
    CFS进程如何均衡进程的虚拟运行时间：当CFS需要选择下一个运行进程时，它会挑一个具有最小vruntime的进程。这其实就是CFS调度算法的核心：选择具有最小vruntime的任务。那么剩下的内容我们来讨论到底是如何实现选择具有最小vruntime值的进程。
    CFS使用红黑树来组织可运行进程队列，并利用其迅速找到最小vruntime值的进程。在Linux中，红黑树称为rbtree，它是一个自平衡二叉搜索树。
    当进程变成可运行状态（被唤醒）或者是通过fork（）调用第一次创建进程时，CFS会将进程加入rbtree中，并缓存最左叶子节点，在CFS调度选择下一任务时变直接选择在缓存中的进程来运行。
    当进程阻塞（变为不可运行态）或者终止时（结束运行）CFS会删除rbtree中的进程。并遍历rbtree找到最新的左节点。
###4.5.3 调度器入口
    进程调度的主要入口点是函数schedule()。它正是内核其它部分用于调用进程调度器的入口：选择哪个进程可以运行，何时将其投入运行。
###4.5.4 睡眠和唤醒
    休眠（被阻塞）的进程处于一个特殊的不可执行的状态。这点非常重要，如果没有这种特殊状态的话，调度程序就可能选出一个本不愿意被执行的进程，更糟糕的是，休眠就必须以轮询的方式实现了。进程休眠有多种原因，但肯定都是为了等待一些事件。事件可能是一段时间从文件I/O读更多数据，或者是某个硬件事件。一个进程还有可能在尝试获取一个已被占用的内核信号量时被迫进入休眠。休眠的一个常见原因就是文件I/O————如果进程对一个文件执行了read()操作，而这需要从磁盘读取。还有，进程在获取键盘输入的时候也需要等待。无论哪种情况，内核的操作都相同：进程把自己标记为休眠状态，从可执行红黑树中移出，放入等待队列，然后调用schedule()选择和执行一个其他进程。唤醒的过程刚好相反：进程被设置为可执行状态，然后再从等待队列中移入到可执行红黑树中。
    1.等待队列
        休眠通过等待队列进行处理。等待队列是由等待某些事件发生的进程组成的简单链表。
    2.唤醒
        唤醒操作通过函数wake_up()进行，它会唤醒指定的等待队列上的所有进程。举例来说，当磁盘数据到来时，VFS就要负责对等待队列调用wake_up()，以便唤醒队列中等待这些数据的进程。
        关于休眠有一点需要注意，存在虚假唤醒。有时候进程被唤醒并不是因为它所等待的条件达成了才需要用一个循环处理来保证它等待的条件真正达成。
##4.6 抢占和上下文切换
    上下文切换，也就是从一个可执行进程切换到另一个可执行进程。sched.c中的context_switch()函数负责上下文切换。每当一个新的进程被选出来准备投入运行的时候，schedule()就会调用该函数，它做了两件事：
        1，调用switch_mm()，把虚拟内存从上一个进程映射切换到新进程中。
        2，调用switch_to()，从上一个进程的处理器状态切换到新进程的处理器状态。这包括保存、恢复栈信息和寄存器信息，还有其他任何与体系结构相关的状态信息，都必须以每个进程为对象进行管理和保存。
###4.6.1 用户抢占
    内核即将返回用户空间的时候，如果need_resched（表名是否需要重新执行一次调度）标志被设置，会导致schedule()被调用，此时就会发生用户抢占。
        用户抢占在一下情况时发生：
            从系统调用返回用户空间时。
            从中断处理程序返回用户空间时。
###4.6.2 内核抢占
    在不支持内核抢占的内核中，内核代码可以一直执行，到它完成为止。也就是说，调度程序没有办法在一个内核级的任务正在执行的时候重新调度————内核中的各任务是以协作方式调度的，不具备抢占性。内核代码一直要执行到完成（返回用户空间）或明显的阻塞为止。
        内核抢占会发生在：
            中断处理程序正在执行，且返回内核空间之前。
            内核代码再一次具有可抢占性的时候。
            如果内核中的任务显式地调用schedule()。
            如果内核中的任务阻塞（这也同样会导致调用schedule())。
##4.7 实时调度策略
    Linux提供了两种实时调度策略：SCHED_FIFO 和 SCHED_RR。而普通的、非实时的调度策略是SCHED_NORMAL。
    卧槽这块儿需要看看书，咋回事儿
##4.8 与调用相关的系统调用
###4.8.1 与调度策略和优先级相关的系统调用
###4.8.2 与处理器绑定有关的系统调用
    在进程描述符中有个字段 cpus_allowed 使进程只运行在指定的处理器上。
###4.8.3 放弃处理器时间
    sched_yield()系统调用。从可运行队列里移出，移入到过期队列里————确保在一段时间内它都不会再被执行了。
#第5章 系统调用
    在现在操作系统中，内核提供了用户进程与内核进行交互的一组接口。这些接口让应用程序受限地访问硬件设备，提供了创建新进程并与已有进程进行通信的机制，也提供了申请操作系统其他资源的能力。提供这些接口主要是为了保证系统稳定可靠，避免应用程序肆意妄想。
##5.1 与内核通信
    系统调用在用户空间进程和硬件设备之间添加了一个中间层。作用有三：
        为用户空间提供了一种硬件的抽象接口
        系统调用保证了系统的稳定和安全
        在第3章中曾经提到过，每个进程都运行在虚拟系统中，而在用户空间和系统的其余部分提供了这样一层公共接口，也是出于这种考虑。
    本章重点强调Linux系统调用的规则和实现方法。
##5.2 API/POSIX/C库 
##5.3 系统调用
    要访问系统调用（在Linux中常称作syscall)，通常通过C库中定义的函数调用来进行。
###5.3.1 系统调用号
    在Linux中，每个系统调用被赋予一个系统调用号。用以关联系统调用。用户空间的进程执行一个系统调用的时候，这个系统调用号就用来指明到底是要执行哪个系统调用：根据系统调用号进行系统调用。
###5.3.2 系统调用的性能
    Linux系统调用比其他许多操作系统执行的要快。Linux很短的上下文切换时间是一个重要的原因，进出内核都被优化的简洁高效。另一个原因是系统调用处理程序和每个系统调用本身也都非常简洁。
##5.4 系统调用处理程序
    用户空间的程序无法直接执行内核代码。它们不能直接调用内核空间中的函数，因为内核驻留在受保护的地址空间上（安全性和稳定性）。
    所以，应用程序应该以某种方式通知系统，告诉内核自己需要执行一个系统调用，希望系统切换到内核态（软中断），这要内核就可以代表应用程序在内核空间执行系统调用。
    通知内核的机制是靠软中断实现的：通过印发一个异常来促使系统切换到内核态去执行异常处理程序。此时的异常处理程序实际上就是系统调用处理程序。
###5.4.1 指定恰当的系统调用
    因为所有的系统调用陷入内核的方式都一样（软中断：通过异常来促使系统切换到内核态执行异常->系统调用程序system_call），所以仅仅是陷入内核空间是不够的。还需要传入系统调用号。（通过寄存器传递给内核）
###5.4.2 参数传递
    除了系统调用号意外，大部分系统调用还需奥一些外部的参数输入。（也通过寄存器传递给内核）。
##5.5 系统调用的实现。
    如何添加一个新的系统调用？
###5.5.1 实现系统调用
###5.5.2 参数验证
##5.6 系统调用上下文
    第3章讨论过，内核在执行系统调用的时候处于进程上下文。current指针指向当前任务，即引发系统调用的那个进程。
    在进程上下文中，内核可以休眠（比如在系统调用阻塞或显式调用schedule()的时候）并且可以被抢占。
###5.6.1 绑定一个系统调用的最后步骤
    如果把自己写的系统调用注册成一个正式的系统调用？
###5.6.2 从用户空间访问系统调用
    
#第6章 内核数据结构
    链表、队列、映射、二叉树
#第7章 中断和中断处理
    任何操作系统内核的核心任务，都包含有对连接到计算机上的硬件设备进行有效管理，如硬盘、蓝光碟机、键盘、鼠标、3D处理器等。因为处理器的速度和外围硬件设备的速度不在一个数量级上，因此，如果内核采取让处理器向硬件发出一个请求，然后专门等待回应的办法，显然差强人意。既然硬件的响应这么慢，那么内核就应该在此期间处理其他事物，等到硬件真正完成了请求的操作之后，再回过头来对她进行处理。
    那么到底如何让处理器和这些外部设备能协同工作，且不会降低机器的整体性能呢？轮询（polling）可能会是一种解决办法。它可以让内核定期对设备的状态进行查询，然后做出相应的处理。不过这种方法很可能会让内核做不少无用功，因为无论硬件设备是正在忙碌着完成任务还是已经大功告成，轮询总会周期性地重复执行。更好的办法是由我们来提供一种机制，让硬件在需要的时候再向内核发出信号（变内核主动为硬件主动，有点像callback模式）。这就是中断机制。
##7.1 中断
    中断使得硬件以发出通知给处理器。例如，在你敲击键盘的时候，键盘控制器会发送一个中断，通知操作系统有键按下。中断本质上是一种特殊的电信号，由硬件设备发向处理器。处理器接收到中断后，会马上向操作系统反映此电信号的到来，然后就由操作系统负责处理这些新到来的数据。内核随时可能因为新到来的中断而被打断。
    从物理学的角度看，中断是一种电信号，由硬件设备生成，并直接送入中断控制器的输入引脚中————中断控制器是个简单的电子芯片，其作用是将多路中断管线，采用复用技术只通过一个和处理器相连接的管线与处理器通信。当接收到一个中断后，中断控制器会给处理器发送一个电信号。处理器一经检测到此信号，便中断自己的当前工作转而处理中断。伺候，处理器会通知操作系统已经产生中断，这样，操作系统就可以对这个中断进行适当的处理了。
    不同的设备对应的中断不同，而每个中断都通过一个唯一的数字标志。操作系统能对中断进行区分，提供相应的中断处理程序处理。
    
    异常
        在操作系统中，讨论中断就不能不提及异常。异常与中断不同，它在产生时必须考虑与处理器时钟同步。实际上，异常也常常称为同步中断。在处理器执行到由于编程失误而导致的错误指令（如被0除）的时候，或者是在执行期间出现特殊情况（如缺页），必须靠内核来处理的时候，处理器就会产生一个异常。中断和异常类似，只不过异常是由软件引起的，中断是由硬件引起的。
##7.2 中断处理程序
    在响应一个特定中断的时候，内核会执行一个函数，该函数叫做中断处理程序（interrupt handler）或中断服务历程（interrupt service routine，ISR）。产生中断的每个设备都有一个相应的中断处理程序。一个设备的中断处理程序是它设备驱动程序（driver）的一部分————设备驱动程序是用于对设备进行管理的内核代码。
    在Linux中，中断处理程序就是普普通通的C函数。中断处理程序与其他内核函数的真正区别在于，中断处理程序是被内核调用来响应中断的。而它们运行于我们称之为中断上下文的特殊上下文中。中断上下文偶尔也称为原子上下文，该上下文的执行代码不可阻塞。
    网络设备的中断处理程序面临的挑战：除了要对硬件应答，告诉硬件设备中断已被接收，还要把来自硬件的网络数据包拷贝到内存，对其进行处理后，再交给合适的协议栈（TCP/IP）或应用程序。  
##7.3 上半部与下半部的对比
    又想中断处理程序运行得快，又想中断处理程序完成的工作量多。鉴于两个目的之间存在此消彼长的矛盾关系，所以我们一般把中断处理切为两个部分或两半。
    中断处理程序是上半部（top half）————接收到一个中断，它就立即开始执行，但只做有严格时限的工作（例如对接收的中断进行应答或复位硬件），能够被允许稍后完成的工作会被推迟到下半部（bottom half）去执行。
    以网卡为例。当网卡接收来自网络的数据包时，需要通知内核数据包到了。网卡需要立即完成这件事，从而优化网络的吞吐量和传输周期，以免超时。因此，网卡立即发出中断：hi，内核，我这里有最新数据包了。内核通过执行网卡已注册的中断处理程序来做出应答。
    中断开始执行，通知硬件，拷贝最新的网络数据包到内存，然后读取网卡更多的数据包。这些都是重要、紧迫而又与硬件相关的工作。内核通常需要快速的拷贝网络数据包到系统内存，因为网卡上接收的网络数据包的缓存大小固定————进入的网络包占满了网卡的缓存，后续的入包只能被丢弃。当网络数据包被拷贝到系统内存后，中断的任务算是完成了，这时它将控制权交还给系统被中断前原先运行的程序。处理和操作数据包的其他工作在随后的下半部中进行。
##7.4 注册中断处理程序
    中断处理程序是管理硬件的驱动程序的组成部分。每一设备都有相关的驱动程序，如果设备使用中断，那么相应的驱动程序就注册一个中断处理程序。
##7.6 中断上下文
    当执行一个中断处理程序时，内核处于中断上下文（interrupt context）。进程上下文是一种内核所处的操作模式，此时内核代表进程执行————例如，执行系统调用或运行内核线程。在进程上下文中，可以通过current 宏关联当前进程。此外，因为进程是以进程上下文的形式连接到内核中的，因此，进程上下文可以睡眠，也可以调用调度程序。
    与之相反，中断上下文和进程并没有什么瓜葛。与current 宏也是不相干的（尽管它会指向被中断的进程）。因为没有后备进程，所以中断上下文不可以睡眠，否则又怎么再对它重新调度呢？因此，不能从中断上下文中调用某些函数。如果一个函数睡眠，就不能在中断处理程序中使用它。
    中断上下文执行有严格的时间限制，因为它打断了其它代码。中断上下文应当迅速、简洁，尽量不要使用循环去处理繁重的工作。请牢记：中断处理程序打断了其它代码甚至其它中断程序。正是因为这种异步执行的特性，所以所有的中断处理程序尽可能迅速、简洁。尽量把工作从中断处理程序分离出来放在下半部中执行，因为下半部可以再更合适的时间运行。
##7.7 中断处理机制的实现
##7.9 中断控制
    Linux内核提供了一组接口用于操作机器上的中断状态。禁止当前处理器的中断系统，或屏蔽掉整个机器的一条中断线的能力。
    一般来说，控制中断系统的原因归根结底是需要提供同步。通过禁止中断，可以确保某个中断处理程序不会抢占当前的代码。此外，禁止中断还可以禁止内核抢占。然而不管是禁止中断还是禁止内核抢占当前代码，都没有提供任何保护机制来防止来自其他处理器的并发访问。Linux支持多处理器，因此，内核代码一般都需要获取锁来防止其他处理器的并发访问，而禁止中断提供的保护机制，则是防止来自其他中断处理程序的并发访问。
###7.9.1 禁止和激活中断

#第8章 下半部和推后执行的工作
    中断处理程序由于一些局限性只能完成整个中断处理流程的上半部分，这些局限包括：
        中断处理程序以异步方式执行，并且它有可能打断其他重要代码的执行（甚至其它中断），因此，为了避免被打断的代码停止时间过长，中断处理程序应该执行的越快越好。
        如果当前有一个中断处理程序正在执行，（没设置IRQF_DISABLED）与该中断同级的其他中断会被屏蔽。（设置了IRQF_DISABLED)，当前处理器上所有其他中断都会被屏蔽。因为禁止中断后硬件与操作系统无法通信，因此中断处理程序越快越好。
        由于中断处理程序往往需要对硬件进行操作，所以它们通常有很高的时限要求。
        中断处理程序不在进程上下文中运行，所以它们不能阻塞。这限制了它们所做的事情。
##8.1 下半部
    下半部的任务就是执行与中断处理程序密切相关但中断处理程序本身不执行的工作。在理想的情况下，最好是中断处理程序将所有工作都交给下半部分执行。
    但是，中断处理程序注定要做一些工作。比如，几乎都需要通过操作硬件对中断的到达进行确认，有时它还会从硬件拷贝数据。因为这些工作对时间非常敏感，所以只能靠中断处理程序自己去完成。
###8.1.1 为什么要用下半部
    使用下半部的原因是什么？
        1，上半部在运行的时候，当前的中断线会被所有处理器屏蔽，被屏蔽就无法响应新的中断信号了，上半部执行时间长，交互系统就会给人"卡"的感觉。
        2，不仅是Linux，许多操作系统也把处理硬件中断的过程分为两部分。上半部分简单快速，执行的时候禁止一些或者全部中断。下半部分稍后执行，并且执行期间可以响应所有的中断。这种设计可使系统处于中断屏蔽状态的时间尽可能的短，以此来提高系统的响应能力。
###8.1.2 下半部的环境
    和上半部只能通过中断处理程序实现不同，下半部可以通过多种机制实现。这些用来实现下半部的机制分别由不同的接口和子系统组成。
        1，bottom half：它提供一个静态创建、由32个bottom halves 组成的链表。上半部通过一个32位整数中的一位来标识出哪个bottom half 可以执行。每个BH都在全局范围内进行同步。即使分属于不同的处理器，也不允许任何两个BH同时执行。这种机制使用方便却不够灵活，简单却有性能瓶颈。
        2，任务队列（先对要推后执行的工作排队，稍后在进程上下文中执行它们）：内核开发者引入task queue机制来实现工作的推后执行，并用它来代替BH机制。内核为此定义了一组队列，其中每个队列都包含一个由等待调用的函数组成链表。根据其所处队列的位置，这些函数会在某个时刻执行。驱动程序可以把它们自己的下半部注册到合适的队列上去。仍不够灵活，不能替代整个BH接口。对于一些性能要求高的子系统，像网络部分，它也不能胜任。
        3，软中断（softirqs  IRQs(interrupt requests)）和tasklet：软中断softirqs是一组静态定义的下半部接口，有32个，可以在所有处理器上同时执行————即使两个类型相同也可以。tasklet是一种基于软中断实现的灵活性强、动态创建的下半部实现机制。两个不同类型的tasklet可以在不同的处理器上同时执行，但类型相同的tasklet不能同时执行。对于大部分下半部处理器来说，用tasklet就足够了，像网络这样对性能和易用性之间寻求平衡的产物。对于大部分下半部处理器来说，用tasklet就足够了像网络这样对性能要求非常高的情况才需要使用软中断。此外，使用软中断需要特别小心，因为两个相同的软中断有可能同时被执行。
        此外，软中断还必须在编译器间就进行静态注册。与此相反，tasklet可以通过代码进行动态注册。
    内核定时器
        另外一个可以用于将工作推后执行的机制是内核定时器。不像上述机制，内核定时器把操作推迟到某个确定的时间段之后执行。也就是说，也就是说，其他机制可以把操作推后到除了现在以外的任何时间进行，但是当你必须保证在一个确定的时间段过去以后再运行时，可以使用内核定时器。
        4，混乱的下半部概念
        下半部 bottom half 是一个操作系统通用词汇，用于指代中断处理流程中推后执行的那一部分。用于实现将工作推后执行的内核机制都被称为"下半部机制"。一些人错误地把所有的下半部机制都叫做"软中断"，真是在自寻烦恼。
        有三种机制可以用来将工作推后执行：软中断、tasklet、工作队列（为什么不提内核定时器？）。tasklet通过软中断实现，而work queue 和他们完全不同。
##8.2 软中断
    软中断使用得比较少：而tasklet是下半部更常用的一种形式。但是由于tasklet是通过软中断实现的，所以我们先看软中断。
###8.2.1 软中断的实现
    软中断是在编译期间静态分配的。它不像tasklet那样能被动态地注册或注销。
    1.软中断处理程序：我理解就是给action传递一个结构体，里面取这个结构体里的值进行执行。
    一个软中断不会抢占另一个软中断。实际上，唯一可以抢占软中断的是中断处理程序。不过，其他的软中断（甚至是相同类型的软中断）可以在其他处理器上同时执行。
    2.执行软中断：一个注册的软中断必须在标记后才会执行（有点像事件驱动？）。这被称作触发软中断。在下列地方，待处理的软中断会被检查和执行：
        从一个硬件中断代码处返回时
        在ksoftirqd内核线程中
        在那些显式检查和执行待处理的软中断代码中，如网络子系统中
        点评：执行软中断程序真没啥，遍历vec，执行action。
###8.2.2 使用软中断
    软中断保留给系统中对时间要求最严格以及最重要的下半部使用。目前，只有两个子系统（网络和SCSI）直接使用软中断。此外，内核定时器和tasklet都是建立在软中断上的。
##8.3 tasklet
    tasklet是用软中断实现的一种下半部机制。它和软中断相比，接口更简单，锁保护也要求较低。
    因为是靠软中断实现，所以tasklet不能睡眠。这意味着你不能在tasklet中使用信号量或者其他什么阻塞式的函数。由于tasklet运行时允许响应中断，所以你必须做好预防工作（如屏蔽中断然后获取一个锁），如果你的tasklet和中断处理程序共享了某些数据的话。两个相同的tasklet决不会同时执行————尽管不同的tasklet可以再两个处理器上同时执行，但是注意共享数据要锁保护
    作为一种优化措施，一个tasklet总在调度它的处理器上执行————这是希望能更好地利用处理器的高速缓存。
    ksoftirqd
        每个处理器都有一组辅助处理软中断（和tasklet）的内核线程。当内核中出现大量软中断的时候，这些内核进程就会辅助处理它们。
        对于软中断，内核会选择在几个特殊时机进行处理。而在中断处理程序返回时处理是最常见的。软中断被处罚频率有时可能很高（像在进行大流量的网络通信期间）。更不利的是，处理函数有时还会自行重复触发。也就是说，当一个软中断执行的时候，它可以重新触发自己以便再次得到执行（比如 网络子系统）。如果软中断本身出现的频率就高，再加上他们又有将自己重新设置为可执行状态的能力，那么就会导致用户空间进程无法获得足够的处理器时间，因而处于饥饿状态。而且，单纯的对重新触发的软中断采取不立即处理的策略，也无法让人接受。那么如何稍微缓解呢？
            1，只要还有被触发并等待处理的软中断，本次执行就要负责处理，重新触发的软中断也在本次执行返回前被处理。这样做可以保证对内核的软中断采取即时处理的方式，关键在于，对重新触发的软中断也会立即处理。当负载很高会出问题。系统一直处理软中断，用户空间任务被忽略。这种方案适用于地复杂的系统下。
            2，不处理重新触发的软中断。在从中断返回的时候，内核和平常一样，也会检查所有挂起的软中断并处理它们。但是，任何自行重新触发的软中断都不会马上处理，它们被放到下一个软中断（上半部）执行时被处理。而这个时机通常也是下次中断返回的时候。可是，在比较空闲的系统中，立即处理软中断才是比较好的做法。这个方案和上面那个又是一个时好时坏的选择。尽管它能保证用户控件不处于饥饿状态，但它却让软中断忍受饥饿的痛苦，而根本没有好好利用闲置资源。
        内核线程的运用：当大量软中断出现的时候，内核会唤醒一组内核线程来处理这些负载（辅助调度软中断）。这些线程在最低的优先级上运行（nice值是19，哇，内核线程的优先级好低啊），这能避免它们跟其他重要的任务抢夺资源，但它们肯定会被执行。现象就是：软中断复旦很重的时候，用户程序不会因为得不到处理器时间而处于饥饿状态。相应的，也能保证"过量"的软中断最终会得到处理。最后，在空闲系统上，这个方案同样表现良好，软中断处理得非常迅速（因为仅存的内核线程肯定会马上调度）。
        
##8.4 工作队列
    工作队列是另外一种将工作推后执行的形式，它和我们前面讨论的形式都不相同。工作队列可以把工作推后，交由内核线程去执行————这个下半部分总是会在进程上下文中执行。这样，通过工作队列执行的代码能占尽进程上下文的所有优势。最重要的就是工作队列允许重新调度甚至是睡眠。
    如果你需要用一个可以重新调度的实体来执行你的下半部处理，你应该使用工作队列。它是唯一能在进程上下文中运行的下半部实现机制。也只有它可以睡眠。这意味着在你需要获得大量的内存时，在你需要获取信号量时，在你需要执行阻塞式的I/O操作时，它都会很有用。否则使用tasklet吧。
###8.4.1 工作队列的实现
    工作队列子系统是一个用于创建内核线程的接口（玩的是内核线程了），通过它创建的进程负责执行由内核其他部分排到队列里的任务。它创建的这些内核线程称作工作者线程（worker thread）。工作队列可以让你的驱动程序创建一个专门的工作者线程来处理需要推后的工作。工作队列可以让你的驱动程序创建一个专门的工作者线程来处理需要推后的工作。
    工作队列子系统提供了一个缺省的工作者线程来处理这些工作。缺省的工作者线程叫做events/n，这里的n是处理器的编号；每个处理器对应一个缺省的工作者线程。缺省的工作者线程会从多个地方得到被推后的工作。许多内核驱动程序都把他们的下半部交给缺省的工作者线程去做。除非驱动程序或者子系统必须建立一个属于自己的内核线程，否则最好用缺省的工作者线程。
    不过并不存在什么东西能够阻止代码创建属于自己的工作者线程。如果你需要工作者线程中执行大量的处理操作，比如处理器密集型和性能要求严格的任务会因为拥有自己的工作者线程而表现更好，并减轻了缺省的工作者线程的负担，避免工作队列中其他需要完成的工作处于饥饿状态。
    
###8.4.2 使用工作队列
    工作队列处理函数的原型是：
    void work_handler(void *data)
    这个函数会由一个工作者线程执行，因此，函数会运行在进程上下文中。默认情况下，允许响应中断，并且不持有任何锁。如果需要，函数可以休眠。BTW，尽管操作处理函数运行在进程上下文中，但它不能访问用户空间，因为内核线程在用户空间没有相关的内存映射。通常在发生系统调用时，内核会代表用户空间的进程运行，此时它可以访问用户空间，也只有在此时它才会映射用户空间的内存。
##8.5 下半部机制的选择
    下半部的比较
    软中断：中断上下文，没有 顺序执行的保障
    tasklet：中断，同类型不能同时执行
    工作队列：进程上下文，没有 顺序执行的保障（和进程上下文一样被调度）。
    有休眠需要的下半部 选择 工作队列。否则最好用tasklet。要是必须专注于性能的的提高，就选择软中断。
##8.6 在下半部之间加锁
    在使用下半部机制时，即使是在一个单处理器的系统上，避免共享数据被同事访问也是至关重要的。
    使用tasklet的一个好处是，它自己负责执行的序列化保障：两个相同类型的tasklet不允许同时执行，即使在不同的处理器上也不行。这意味着你无须为intra-tasklet的同步问题操心了。

#第9章 内核同步介绍
    在使用共享内存的应用程序中，程序员必须特别留意保护共享资源，防止共享资源并发访问。内核也不例外。共享资源之所以要防止并发访问，是因为如果多个执行线程同时访问和操作数据，就有可能发生各线程之间相互覆盖共享数据的情况，造成被访问数据处于不一致的状态。
    本章讨论操作系统内核中的并发和同步问题。
##9.1 临界区和竞争条件
    所谓临界区就是访问和操作共享数据的代码段。多个执行线程并发访问同一个资源通常是不安全的，为了避免在临界区中并发访问，编程者必须保证这些代码原子地执行————也就是说，操作在执行结束前不可被打断，就如同整个临界区是一个不可分割的指令一样。如果两个执行线程有可能处于同一个临界区中同时执行，那么这就是程序包含的一个bug。如果这种情况确实发生了，我们就称它是竞争条件（race conditions），这样命名是因为这里会存在线程竞争。避免并发和防止竞争条件称为同步。
###9.1.1 为什么我们需要保护
    操作期间对账户加锁，确保每个事物相对其他任何事物的操作是原子性的。这样的事物必须完整地发生，要么干脆不发生，但是决不能打断。
###9.1.2 单个变量
    原子操作交错执行不会发生并发问题，因为处理器会从物理上确保这种不可能。使用这样的指令会缓解这种问题，内核也提供了一组实现这些原子操作的接口。
##9.2 加锁
    以队列的插入和读取为例。一个处理器读取队列的时候，我们怎么能禁止另一个处理器更新队列呢？虽然有些体系结构提供了简单的原子指令，实现算术运算和比较之类的原子操作，但让体系结构提供专门的指令，对队列这种不定长度的临界区进行保护，就强人所难了。我们需要一种方法确保一次有且只有一个线程对数据结构进行操作，或者当另一个线程在堆临界区标记时，就禁止（或者说锁定）其他访问。
    锁提供的就是这种机制：它如同一把门锁，门后的房间可想象成一个临界区。在一个指定的时间内，房间里只能有一个执行线程存在，当一个线程进入房间后，它会锁住身后的房门：当它结束对共享数据的操作后，就会走出房间，打开门锁。如果另一个线程在房门上锁时来了，那么它就必须等待房间内的线程出来并打开门锁后，才能进入。这样，线程持有了锁，而锁保护了数据。
    请注意锁的使用是自愿的、非强制的，它完全属于一种编程者自选的编程手段。
    锁有多种多样的形式，而且加锁的粒度范围也各不相同————Linux自身实现了几种不同的锁机制。各种锁机制之间的区别主要在于：当锁已经被其他线程持有，因而不可用时的行为表现————一些锁被征用时会简单地执行忙等待（也就是说，反复处于一个循环中，不断检测锁状态，等待锁变为可用），而另外一些锁会使当前任务睡眠直到锁可用为止。
    机灵的读者此时会尖叫起来，锁根本解决不了什么问题，它只不过是把临界区缩小到加锁和开锁之间的代码，但是仍然有潜在的竞争！所幸，锁是采用原子操作实现的，而原子操作不存在竞争。
###9.2.1 造成并发执行的原因
    用户空间之所以需要同步，是因为用户程序会被调度程序抢占和重新调度。由于用户进程可能在任何时刻被抢占，而调度程序完全可能选择另一个高优先级的进程到处理器上执行，所以就会使得一个程序正处于临界区时，被非自愿地抢占了（临界区被抢占，md，释放不了锁啊，那别的线程咋用呢？但是此例没用锁哦）。如果新调度的进程随后也进入同一个临界区（比如，这两个进程要操作共享的内存，或者向同一个文件描述符中写入），前后两个进程相互之间就会产生竞争。这种类型的并发操作————这里其实两者并不真的同时发生的，但它们相互交叉进行，所以也可称作伪并发执行。
    如果是多处理器的机器，那么两个进程就可以真正地在临界区中同时执行了，这种类型称为真并发。
    内核中有类似可能造成并发执行的原因。它们是：
        1，中断————中断几乎可以再任何时刻异步发生，也就可能随时打断当前正在执行的代码。
        2，软中断和tasklet————内核能在任何时候唤醒或调度软中断和tasklet，打断当前正在执行的代码。
        3，内核抢占————因为内核具有抢占性，所以内核中的任务可能会被另一任务抢占。
        4，睡眠及与用户空间的同步————在内核执行的进程可能会睡眠，这就会唤醒调度程序，从而导致调度一个新的用户进程执行。
        5，对称多处理————两个或多个处理器可以同时执行代码。
        如果在一段内核代码操作某资源的时候系统产生了一个中断，而且该中断的处理程序还要访问这一资源，这就是bug；类似的，如果一段内核代码在访问一个共享资源期间可以被抢占，这也是一个bug；还有，如果内核代码在临界区里睡眠，那简直就是鼓掌欢迎竞争条件的到来。最后还要注意，两个处理器绝对不能同时访问同一共享数据。当我们清楚什么样的数据需要保护时，提供锁来保护系统稳定也就不难做到了。
        其实，用锁来保护共享资源并不困难，辨认出真正需要共享的数据和相应的临界区，才是真正有挑战的地方。在编写代码的开始阶段就要设计恰当的锁。
        在中断处理程序中能避免并发访问的安全代码称作中断安全代码（interrupt-saft），在对称多处理的机器中能避免并发访问的安全代码称为SMP安全代码（SMP-saft），在内核抢占时能避免并发访问的安全代码称为抢占安全代码（preempt-safe）。
###9.2.2 了解保护些什么
    由于任何可能被并发访问的代码都几乎无例外地需要保护，所以寻找哪些代码不需要保护反而相对更容易些，我们也就从这里入手。执行线程的局部数据仅仅被它本身访问，显然不需要保护，比如，局部自动变量（还有动态分配的数据结构，其地址存放在堆栈中）不需要任何形式的锁，因为它们独立存在于执行线程的栈中。
    到底什么数据需要加锁呢？大多数内核数据结构都需要加锁！有一条很好的经验可以帮助我们判断：如果有其他执行线程可以访问这些数据，那么就给这些数据加上某种形式的锁：如果任何其他什么东西都能看到它，那么就要锁住它。记住：要给数据而不是代码加锁。
    在写内核代码时，你要问自己下面这些问题：
        1，这个数据是不是全局的？除了当前线程外，其他线程能不能访问它？
        2，这个数据会不会在进程上下文和中断上下文中共享？它是不是要在两个不同的中断处理程序中共享？
        3，进程在访问数据时可不可能被抢占？被调度的新程序会不会访问同一数据？
        4，当前进程是不是会睡眠（阻塞）在某些资源上，如果是，它会让共享数据处于何种状态？
        5，怎样防止数据失控？
        6，如果这个函数又在另一个处理器上被调度将会发生什么呢？
        7，如何确保代码远离并发威胁呢？
##9.3 死锁
    死锁的产生需要一定条件：要有一个或多个执行线程和一个或多个资源，每个线程都在等待其中的一个资源，但所有的资源都已经被占用了。所有线程都在相互等待，但它们永远不会释放已经占有的资源。于是任何线程都无法继续，死锁诞生了。
    ABBA死锁：线程1持有A想获得B锁..
    一些简单的规则对避免死锁大有帮助：
        1，按顺序加锁。使用嵌套的锁时必须保证以相同的顺序获取锁，这样可以阻止致命拥抱类型的死锁。最好能记录下锁的顺序，以便其他人也能照此顺序使用。
        2，防止发生饥饿。试问，这个代码的执行是否一定会结束？如果"张"不发生？"王"要一直等待下去吗？
        3，不要重复请求同一个锁。
        4，设计应力求简单————越复杂的加锁方案越有可能造成死锁。
##9.4 争用和扩展性
    锁的争用（lock contention），或简称争用，是指当锁正在被占用时，有其他线程试图获得该锁。说一个锁处于高度争用状态，就是指有多个其他线程在等待获得该锁。由于锁的作用是使程序以串行方式对资源进行访问，所以使用锁无疑会降低系统的性能。被高度争用（频繁被持有，或者长时间持有————两者都有就更糟糕）的锁会成为系统的瓶颈，严重降低系统性能。即使这样，相比于被几个相互争夺共享资源的线程撕成碎片，搞得内核崩溃，还是这种同步保护来得更好一点。当然，如果有办法解决高度争用的问题，就更好不过了。
    扩展性（scalability）是对系统可扩展成都的一个量度。对于操作系统，我们谈及可扩展性时就会和大量进程、大量处理器或是大量内存等联系起来。其实任何可以被计量的计算机组件都可以涉及可扩展性。理想情况下，处理器的数量加倍，应该会使系统性能翻倍。而实际上，这是不可能达到的。
    同步保护资源的锁粒度越精细越好吗？不是的（细粒度：fine-grained）。加锁粒度用来描述加锁保护的数据规模。一个过粗的锁保护大块数据，比如一个完整的子系统（数据库表锁？），一个过于精细的锁保护很小的一块数据比如一个独立元素（行锁？）。在实际使用中，绝大多数锁的加锁范围都处于上述两种极端之间，保护的既不是一个完整的子系统也不是一个独立元素，而可能是一个单独的数据结构。许多锁的设计在开始阶段都很粗，但是当锁的争用问题变得严重时，设计就向更加精细的加锁方向进化。（高并发经验，是不是应该会加锁！所以currentHashMap其实就是为了学会加锁）。
    在拥有集群处理器机器上，当各个处理器需要频繁访问该链表的时候，只用单独一个锁（比如hashtable）却成了扩展性（比如由于加锁后，多处理器访问该hashtable是串行的，那么最棒的扩展性是：cpu翻倍，性能翻倍。但是目前却是，cpu翻倍仍是串行，所以说是可扩展性的瓶颈）的瓶颈。
    为解决这个瓶颈，我们将原来加锁的整个链表变成为链表中的每一个节点都加入自己的锁，这样一来，如果要对结点进行读写，必须先得到这个结点对应的锁。将加锁粒度变细后，多处理器访问同一个结点时，只会争用一个锁。可是这时锁的争用仍然没有完全避免，那么，能不能为每个节点中的每个元素都提供一个锁呢？答案是：不能。严格地讲，即使这么细的锁可以在大规模SMP机器上执行得很好，但它在双处理器机器上的表现又会怎样呢？如果在双处理器机器锁争用表现得并不明显，那么多余的锁会加大系统开销，造成很大的浪费。
    不管怎么说，可扩展性都是很重要的，需要慎重考虑。关键在于，在设计锁的开始阶段就应该考虑到要保证良好的扩展性。因为即使在小型机器上，如果对重要资源锁得太粗，也很容易造成系统性能瓶颈。锁加得过粗或过细，差别往往只在一线之间。当锁争用严重时，加锁太粗会降低可扩展性：而锁争用不明显时，加锁过细会加大系统开销，带来浪费，这两种情况都会造成系统性能下降。但要记住：设计初期加锁方案应该力求简单，仅当需要时再进一步细化锁的方案。
    精髓在于力求简单。
##9.5 小结
    要编写SMP安全代码，不能等到编码完成后才考虑如何加锁。恰当的同步（也就是加锁）（既要满足不死锁、可扩展，而且还要清晰、简洁）需要从头到尾，在整个编码过程中不断考虑与完善。
    无论写什么代码，首先应该考虑的就是保护数据不被并发访问，记住，加锁你的代码。

#第10章 内核同步方法
    第9章讨论了竞争条件为何会产生以及怎么去解决。第10章将讨论如何为SMP、内核抢占和其他各种情况提供充分的同步保护，确保数据在任何机器和配置中的安全。
    
##10.1 原子操作
    原子操作是其他同步方法的基石。原子操作可以保证指令以原子的方式执行————执行过程不被打断。众所周知，原子原本指的是不可分割的微粒，所以原子操作也就是不能够被分割的指令。例如，第9章提到过的原子方式的加操作，它通过把读取和增加变量的行为包含在一个单步中执行，从而防止了竞争的发生保证了操作结果总是一致的。
    两个原子操作绝对不可能并发地访问同一个变量，这样加操作也就绝不可能引起竞争。锁内存总线的指令，这就确保了其他改变内存的操作不能同时发生。
    内核提供了两组原子操作接口———— 一组针对整数进行操作，另一组针对单独的位进行操作。
    真正的原子操作需要的是————所有中间结果都正确无误。
    问：AtomicInteger是如何保证原子性的，能编译成cpu支持的原子指令。
###10.1.1 原子整数操作（atomic_t.h 和 atomic64_t.h）
    为什么用新的结构体atomic_t 而不是c语言中的int类型，原因有2：
        原子函数只接收atomic_t类型的操作数，可以确保原子操作只与这种特殊类型数据一起使用。
        保证该类型的数据不会被传递给任何非原子函数。
        其次原因：使用atomic_t类型确保编译器不对相应的值进行访问优化————这点使得原子操作最终接收到正确的内存地址，而不只是一个别名。
    
####原子性与顺序性的比较
    一个字长的读取总是原子地发生，绝不可能对同一个字交错地进行写：读总是返回一个完整的字（一个字 貌似指的是一个指针指向的一段32位地址），这或者发生在写操作之前，或者之后，绝不可能发生在写的过程中。
    也许代码比这有更多的要求。或许要求读必须在待定的写之前发生————这种需求其实不属于原子性要求，而是顺序要求。原子性确保指令执行期间不被打断，要么全部执行完，要么根本不执行。另一方面，顺序性确保即使两条或多条指令出现在独立的执行线程中，甚至独立的处理器上，它们本该的执行顺序却依然要保持。本小节讨论的是原子操作只保证原子性。顺序性通过屏障（barrier）指令来实施，在后面讨论。
    在编写代码的时候，能使用原子操作时，就尽量不要使用复杂的加锁机制。对多数体系结构来讲，原子操作与更复杂的同步方法相比较，给系统带来的开销小，对高速缓存行（cache-line）的影响也小。但是，对于那些有高性能要求的代码，对多种同步方法进行测试比较，不失为一种明智的做法。
###10.1.3 原子位操作
    除了原子整数操作外，内核也提供了一组针对位这一级数据进行操作的函数。定义在bitops.h中。
    令人感到奇怪的是位操作函数是对普通的内存地址进行操作的。它的参数是一个指针和一个位号，第0位是给定地址的最低有效位。在32位机上，第31位是给定地址的最高有效位而第32位是下一个字的最低有效位。虽然使用原子位操作在多数情况下是对一个字长的内存进行访问，因而位号应该位于0~31（在64位机器上是 0~63），但是，对参数位号的范围并没有限制。
    真正的原子操作需要的是————所有中间结果都正确无误。
    例如，假定给出两个原子位操作：先对某位置位，然后清零。如果没有原子操作，那么，这一位可能的确清零了，但是也可能根本没有置位。置位操作可能与清除操作同时发生，但没有成功。清除操作可能成功了，这一位如愿呈现为清0.但是，有了原子操作，置位会真正发生，可能有那么一刻，读操作显示所置的位，然后清除操作才执行，该位变为0了。
##10.2 自旋锁
    如果每个临界区都能像增加变量这样简单就好了，可惜现实总是残酷的。现实世界里，临界区甚至可以跨越多个函数。举个例子，我们经常会碰到这种情况：先得从一个数据结构中移出数据，对其进行格式转换和解析，最后再把它加入到另一个数据结构中。整个执行过程必须是原子的，在数据被更新完毕前，不能有其他代码读取这些数据。显然，简单的原子操作对此无能为力，这就需要更为复杂的同步方法————锁来提供保护。
    自旋锁：自旋锁最多只能被一个可执行线程持有。如果一个执行线程试图获得一个被已经持有（即所谓的争用）的自旋锁，那么该线程就会一直进行忙循环————旋转————等待锁重新可用。要是锁未被争用，请求锁的执行线程便能立刻得到它，继续执行。在任意时间，自旋锁都可以防止多余一个的执行线程同时进入临界区。同一个锁可以用在多个位置，例如，对于给定数据的所有访问都可以通过这一把锁得到保护和同步。
        一个被争用的自旋锁使得请求它的线程在等待锁重新可用时自选（特别浪费处理器时间），这种行为是自旋锁的要点。所以自旋锁不应该被长时间持有（持有者嗷，注意了！）。事实上，这点正是使用自旋锁的初衷：在短期间内进行轻量级加锁。还可以采取另外的方式来处理对锁的争用：让请求线程睡眠，直到锁重新可用时再唤醒它。这样处理器就不必循环等待，可以去执行其他代码。这也会带来一定的开销————这里有两次明显的上下文切换，被阻塞的线程要换出和换入，与实现自旋锁的少数几行代码相比，上下文切换当然有较多的代码（更消耗cpu？那么如果自旋时间大于两次上下文切换，那么就没必要自旋了）。当然我们大多数人都不会无聊到去测量上下文切换的耗时，所以我们让持有自旋锁的时间应尽可能的短就可以了。
###10.2.1 自旋锁方法
    自旋锁的实现和体系结构密切相关，代码往往通过汇编实现。如果禁止内核抢占，那么在编译时自旋锁会被完全剔除出内核。
    警告：自旋锁是不可递归的！
    
    自旋锁可以使用在中断处理程序中（此处不能使用信号量，因为它们会导致睡眠）。在中断处理程序中使用自旋锁时，一定要在获取锁之前，首先禁止本地中断（在当前处理器上的中断请求），否则，中断处理程序就会打断正持有锁的内核代码，有可能会视图去争用这个已经被持有的自旋锁。这样一来，中断处理程序就会自选，等待该锁重新可用，但是锁的持有者在这个中断处理程序执行完毕前不可能运行。这就是 双重请求死锁（看来持有锁的线程不能在其它cpu上运行）。注意，需要关闭的只是当前处理器上的中断。如果中断发生在不同的处理器上，即使中断处理程序在同一锁上自旋，也不会妨碍锁的持有者（在不同处理器上）最终释放锁。
    
    锁什么？
        使用锁的时候一定要对症下药，要有针对性。要知道需要保护的是数据而不是代码。尽管本章的例子讲的都是保护临界区的重要性，但是真正保护的其实是临界区中的数据，而不是代码。
    大原则：
        针对代码加锁会使得程序难以理解，并且容易引发竞争条件，正确的做法应该是对数据而不是代码加锁。既然不是对代码加锁，那就一定要用特定的锁来保护自己的共享数据。例如，"struct foo 由 loo_lock 加锁"。武林你何时需要访问共享数据，一定要先保证数据是安全的。而保证数据安全往往就意味着在对数据进行操作前，首先占用恰当的锁，完成操作后再释放它。
##10.3 读写自旋锁
    有时，锁的用途可以明确地分为读取和写入两个场景。例如对一个链表可能既要更新又要检索。当更新链表时，不能有其他代码并发地写链表或从链表中读取数据，写操作要求要求完全互斥。另一方面，当对其检索（读取）链表时，只要其他程序不对链表进行写操作就行了。只要没有写操作，多个并发的读操作都是安全的。任务链表的存取模式（第三章）就是通过读——写自旋锁获得保护的。
    在Linux读——写锁时，最后要考虑的一点是这种锁机制更照顾读锁。当读锁被持有时，写操作为了互斥访问只能等待，但是读者却可以继续成功地占用锁。而自旋等待的写者在所有读者释放锁之前是无法获取锁的。所以，大量读者必定会使挂起的写者处于饥饿状态，设计锁时一定要注意。
    自旋锁提供了一种快速简单的锁实现方法。如果加锁时间不长并且代码不会睡眠（比如中断处理程序），利用自旋锁是最佳选择。如果加锁时间（锁住数据的时间，所以一般按照这个来看！锁住时间不长就用自旋锁嗷）可能很长或者代码在持有锁时有可能睡眠（被切换算吗？），那么最好使用信号量来完成加锁功能。
##10.4 信号量 semaphore
    Linux中的信号量时一种睡眠锁。如果有一个任务试图获得一个不可用（已经被占用）的信号量时，信号量会将其推进一个等待队列，然后让其睡眠。这时处理器能重获自由，从而去执行其他代码。当持有的信号量可用（被释放）后，处于等待队列中的那个任务将被唤醒，并获得该信号量。
    这就比自旋锁提供了更好的处理器利用率，因为没有把时间花费在忙等待上，但是，信号量比自旋锁有更大的开销。
    我们可以从信号量的睡眠特性得出一些有意思的结论：
        1，由于争用信号量的进程在等待锁重新变为可用时会睡眠，所以信号量适用于锁会被长时间持有的情况。
        2，相反，锁被短时间持有时，使用信号量就不太适宜了。因为睡眠、维护等待队列以及唤醒所花费的开销可能比锁被占用的全部时间还要长。
        3，由于执行线程在锁被争用时会睡眠，所以只能在进程上下文中才能获取信号量锁，因为在中断上下文中是不能进行调度的。
        4，你可以在持有信号量时去睡眠（当然你也可能并不需要睡眠），因为当其他进程视图获得同意信号量时不会因此而死锁（因为该进程也只是去睡眠而已，而你最终会继续执行的）。
        5，在你占用信号量的同时不能占用自旋锁。因为在你等待信号量时可能会睡眠，而在持有自旋锁是不允许睡眠的。
##10.5 读 - 写信号量
    读写信号量和 读写自旋锁一样，除非代码中的读和写可以明白无误地分割开来，否则最好不使用它。
##10.6 互斥体
    为了找到一个更简单睡眠锁，内核开发者们引入了互斥体（mutex）。
    特点：
        1，任何时刻只有一个任务可以持有mutex，也就是说，mutex的使用计数永远是1.
        2，给mutex上锁者必须负责给其再解锁————你不能在一个上下文中锁定一个mutex，而在另一个上下文中给它解锁。这个限制使得mutex不适合内核同用户空间复杂的同步场景。最常使用的方式是：在同一上下文中上锁和解锁。
        3，递归地上锁和解锁是不允许的。
        4，当持有一个mutex时，进程不可以退出。
        5，mutex不能在中断或者下半部中使用，即使使用mutex_trylock()也不行。
##10.7 完成变量（ completion variable）
    如果在内核中一个任务需要发出信号通知另一个任务发生了某个特定事件，利用完成变量是使两个任务得以同步的简单方法。例如，当子进程执行或退出时，vfork()系统调用使用完成变量唤醒父进程。
##10.8 BLK：大内核锁
    欢迎来到内核的原始混沌时期。BLK（大内核锁）是一个全局自旋锁，使用它主要是为了方便实现从Linux最初的SMP过渡到细粒度加锁机制。
##10.9 顺序锁（特别像乐观锁）
    顺序锁，通常简称seq锁。这种锁提供了一种很简单的机制（机制是什么？序列计数器），用于读写共享数据。实现这种锁主要依靠一个序列计数器。当有疑义的数据被写入时，会得到一个锁，并且序列值会增加。在读取数据之前和之后，序列号都被读取。如果读取的序列号值相同，说明在读操作进行的过程中没有被写操作打断过。此外，如果读取的值是偶数，那么就表明写操作没有发生（要明白因为锁的初值是0，所以写锁会使值变成奇数，释放的时候变成偶数）。
##10.10 禁止抢占
    由于内核是抢占性的，内核中的进程在任何时刻都可能停下来以便另一个具有更高优先权的进程运行。这意味着一个任务与被抢占的任务可能会在同一个临界区内运行。为了避免这种情况，内核抢占代码使用自旋锁作为非抢占区域的标记。如果一个自旋锁被持有，内核便不能进行抢占。因为内核抢占和SMP面对相同的并发问题，并且内核已经是SMP安全的（SMP-safe），所以，这种简单的变化使得内核也是抢占安全的（preempt-safe）。
    或许这就是我们希望的。实际中，某些情况并不需要自旋锁，但是仍然需要关闭内核抢占。最频繁出现的情况就是每个处理器上的数据。如果数据对每个处理器是唯一的，那么，这样的数据可能就不需要使用锁来保护，因为数据只能被一个处理器访问。如果自旋锁没有被持有，内核又是抢占式的，那么一个新调度的任务就可能访问同一个变量，如下所示：
        通常情况下，会为这个变量配置自旋锁来避免并发问题。但是如果这是每个处理器上独立的变量，可能就不需要锁。为了解决这个问题，可以通过preempt_disable()函数禁止内核抢占。调用preempt_enable()，内核抢占才重新启用。
        抢占计数存放着被持有锁的数量和preempt_disable()的调用次数，如果计数是0，那么内核可以进行抢占：如果为1或更大的值，那么，内核就不会进行抢占。这个计数非常有用————它是一种对原子操作和睡眠很有效的调试方法。
##10.11 顺序和屏障
    当处理多处理器之间或硬件设备之间的同步问题时，有时需要在你的程序代码中以指定的顺序发出读内存（读入）和写内存（存储）指令。在和硬件交互时，时常需要确保一个给定的读操作发生在其他读或写操作之前。另外，在多处理器上，可能需要按写数据的顺序读数据（通常确保后来以同样的顺序进行读取）。但是编译器和处理器为了提高效率，可能对读和写重新排序，这样无疑使问题复杂化了。幸好，所有可能重新排序和写的处理器提供了机器指令来确保顺序要求。同样可以指示编译器不要对给定点周围的指令序列进行重新排序。这些确保顺序的指令称作屏障（barriers）（硬件指令、软件指令）。
    在某些处理器上存在以下代码：
    a=1;
    b=2;
    有可能会在a中存放新值之前就在b中存放新值。
    编译器和处理器都看不出a和b之间的关系。编译器会在编译时按这种顺序编译，这种顺序会是静态的，编译的目标代码就只把a放在b之前。但是，处理器会重新动态排序，因为处理器在执行指令期间，会在取指令和分派时，把表面上看似无关的指令按自己认为最好的顺序排列。
    
    但是处理器和编译器绝不会对下面的代码重新排序：
    a=1；
    b=a；
    此处a和b均为全局变量，因为a与b之间有明确的数据依赖关系。
    但是不管是编译器还是处理器都不知道其他上下文中的相关代码。偶然情况下，有必要让写操作被其他代码识别，也让锁期望的指定顺序之外的代码识别。这种情况常常发生在硬件设备上，但是在多处理器机器上也很常见。
    
    有几个例子 需要好好看看，看书，这里略过 
    
    指令重排序的发生是因为现代处理器为了优化其传送管道（pipeline），打乱了分派和提交指令的顺序。内存屏障告诉处理器在继续执行前提交所有尚未处理的载入或存储指令。
    
#第11章 定时器和时间管理
    时间管理在内核中占有非常重要的地位。相对于事件驱动（更准确的说，时间驱动也是事件驱动的一种————时间的流逝本身就是一种事件。）而言，内核中有大量的函数都是基于时间驱动的。其中有些函数是周期执行的，像对调度程序中的运行队列进行平衡调整或对屏幕进行刷新这样的函数，都需要定期执行，比如每秒100次；而另外一些函数，比如需要推后执行的磁盘I/O等操作，则需要等待一个相对时间后才运行，比如，内核会在500ms后再执行某个任务。
    周期性产生事件————比如每10ms一次————都是由系统定时器驱动的。系统定时器是一种可编程硬件芯片，它能以固定频率产生中断。该中断就是所谓的定时器中断，它所对应的中断处理程序负责更新系统时间，也负责执行需要周期性运行的任务。系统定时器和时钟中断处理程序是Linux系统内核管理机制的中枢，本章重点讨论。另一个关注的焦点是动态定时器————一种用来推迟执行程序的工具。
##11.1 内核中的时间概念
    时间概念对计算机来说有些模糊，事实上内核必须在硬件的帮助下才能计算和管理时间。硬件为内核提供了一个系统定时器用以计算流逝的时间，该时钟在内核中可看成是一个电子时间资源，比如数字时钟或处理器频率等。系统定时器以某种频率自行触发时钟中断，该频率可以通过编程预定，称作街拍率（tick rate）。当时钟中断发生时，内核就通过一种特殊的中断处理程序对其进行处理。
    因为预编的节拍率对内核来说是可知的，所以内核知道连续两次时钟中断的间隔时间。内核靠这种已知的时钟中断间隔来计算墙上时间和系统运行时间。
    举例一些利用时间中断周期执行的工作：
        1，更新系统运行时间。2，更新实际时间。3，在smp系统上，均衡调度程序中各处理器上的运行队列。4，检查当前进程是否用尽了自己的时间片。如果用尽，就重新调度。5，运行超时的动态定时器。6，更新资源消耗和处理器时间的统计值。
##11.2 节拍率：HZ
    系统定时器频率（节拍率）是通过静态预处理定义的，也就是HZ（赫兹）。各个体系结构的HZ值不同，很多都是100HZ，也就是说每秒钟时钟中断100次。
###11.2.1 理想的HZ值
    在2.5开发版内核中，中断频率被提高到1000HZ。下面分析系统定时器使用高频率与使用低频率各有哪些优势
    提高节拍率意味着时钟中断产生得更加频繁，所以中断处理程序也会更频繁地执行。如此一来会给整个系统带来如下好处：
        1，更高的时钟中断解析度（resolution）可提高时间驱动事件的解析度。
        2，提高了时间驱动事件的准确度（accuracy）。
    虽然内核可以提供频度为1ms的时钟，但是并没有证据显示对系统中所有程序而言，频率为1000HZ的时钟频率比频率为100HZ的时钟都更合适。
    另外，提高解析度的同时也提高了精准度。假定内核在某个随机时刻触发定时器，而它可能在任何时间超时，但由于只有在时钟中断到来才能执行它，所以平均误差大约为半个时钟中断周期。比如时钟周期为HZ=100，那么事件平均在设定时刻的+/-5ms内发生，所以平均误差为5ms。如果HZ=1000，平均误差可降低到0.5ms————准确度提高了10倍。
###11.2.2 高HZ的优势
    更高的时钟中断频度和更高的准确度又会带来如下优点：
        1，内核定时器以更高的频度和更高的准确度运行。
            1.1 依赖定时器值执行的系统调用，比如poll()和select()，能够以更高的精度运行。 对poll()和select()超时精度的提高会给系统性能带来极大的好处。频繁使用这两种系统调用的应用程序，往往在等待时钟中断上浪费大量的时间，而事实上，定时值可能早就超时了。（平均误差，可能浪费的时间，是时钟中断周期的一半）。
        2，对注入资源消耗和系统运行时间等的测量会有更精细的解析度。 
        3，提高进程抢占的准确度。同时还会加快调整响应时间。第四章提到过，时钟中断处理程序负责减少当前进程的时间片计数。当时间片计数跌到0时，而又设置了need_resched标志的话，内核便立即重新运行调度程序。假定有一个正在运行的进程，它的时间片只剩下2ms了，此时调度程序又要求抢占该进程，然后去运行另一个新进程；然而，该抢占行为不会再下一个时钟中断到来前发生，也就是说，在这2ms内不可能进行抢占，所以新进程也就可能要比要求的晚10ms才能执行。当然，进程之间也是平等的，因为所有的进程都一视同仁的待遇，调度起来都不准确————但关键不在于此。问题在于由于耽误了抢占，所以对于类似于填充音频缓冲区这样有严格时间要求的任务来说，结果是无法接受的。
###11.2.3 高HZ的劣势
    把节拍率提高到1000HZ会带来一个大问题：节拍率越高，意味着时钟中断频率越高，也就意味着系统负载越重。因为处理器必须花时间来执行时钟中断处理程序，所以节拍率越高，中断处理程序占用的处理器时间越多。这样不但减少了处理器处理其他工作的时间，而且还会更频繁地打乱处理器高速缓存并增加耗电。将始终频率从100HZ提高到1000HZ必然会使时钟中断的负载增加10倍。可是增加前的系统负载又是多少呢？最后的结论是：至少在现代计算机系统上，始终频率为1000HZ不会导致难以接受的负担，并且不会对系统造成较大的影响。
##11.4 硬时钟和定时器
###11.4.1 实时时钟
    实时时钟（RTC）是用来持久存放系统时间的设备，即便系统关闭后，它也可以靠主板上的微型电池提供的电力保持系统的计时。在PC体系结构中，RTC和CMOS集成在一起，而且RTC的运行和BIOS的保存设置都是通过同一个电池供电的。在系统启动时，内核通过读取RTC来初始化墙上时间。
###11.4.2 系统定时器
    系统定时器是内核定时机制中最为重要的角色。系统定时器的根本思想没有区别————提供一种周期性触发中断机制。有些体系结构是通过对电子晶振进行分频来实现系统定时器，还有些体系结构提供了一个衰减测量器（decrementer）————衰减测量器设置了一个初始值，该值以固定频率递减，当减到零时，触发一个中断，但效果都一样。
##11.5 时钟中断处理程序
    时钟中断处理程序可以划分为两个部分：体系结构相关部分和体系结构无关部分。（例程的作用类似于函数，但含义更为丰富一些。例程是某个系统对外提供的功能接口或服务的集合。比如操作系统的API、服务等就是例程）
    与体系结构相关的例程作为系统定时器的中断处理程序而注册到内核中，以便在产生时钟中断时，它能够相应地运行。
##11.6 实际时间
    当前实际时间（墙上时间）。
##11.7 定时器
    定时器（有时也称为动态定时器或内核定时器）是管理内核流逝的时间的基础。内核经常需要推后执行某些代码，比如下半部机制就是为了将工作放到以后执行。但不幸的是，之后这个概念很含糊，下半部的本意并非是放到以后的某个时间去执行任务，而仅仅是不在当前时间执行就可以了。我们需要的是一种工具，能够使工作在指定时间点上执行————不长不短，正好在希望的时间点上。内核定时器证书解决这个问题的理想思路。 
    
###11.7.2 定时器竞争条件
    因为定时器与当前执行代码是异步的，因此就有可能存在潜在的竞争条件。因为内核异步执行中断处理程序，所以应该重点保护定时器中断处理程序中的共享数据。定时器数据的保护问题曾在第8章和第9章讨论过。
###11.7.3 实现定时器
    run_timer_softirq()函数处理软中断TIMER_SOFTIRQ，从而在当前处理器上运行所有的（如果有的话）超时（到点了的）定时器。
    虽然所有的定时器以链表形式存放在一起，但是让内核经常为了寻找超时定时器而遍历整个链表是不明智的。同样，将链表以超时时间进行排序也是很不明智的做法，因为这样一来在链表中插入和删除定时器都会很费时。为了提高搜索效率，内核将定时器按他们的超时时间划分为五组。当定时器超时时间接近时，定时器将随组一起下移。采用分组定时器的方法可以再执行软中断的多数情况下，确保内核尽可能减少搜索超时定时器所带来的的负担。因此定时器管理代码是非常高效的。
##11.8 延迟执行
    内核代码（尤其是驱动程序）除了使用定时器或下半部机制以外，还需要其他办法来推迟执行任务。这种推迟通常发生在等待硬件完成某些工作时，而且等待的时间往往非常短，比如，重新设置网卡的以太模式需要花费2ms，所以在设定网卡速度后，驱动程序必须至少等待2ms才能继续执行。
    内核提供了许多延迟方法处理各种延迟要求。不同的方法有不同的处理特点，有些是在延迟任务时挂起处理器，防止处理器执行任何实际工作；另一些不会挂起处理器，所以也不能确保被延迟的代码能够在指定的延迟时间运行。
###11.8.1 忙等待
    最简单的延迟方法（虽然通常也是最不理想的办法）是忙等待（或者说忙循环）。但是要注意该方法仅仅在想要延迟的时间是节拍的整数倍，或者精确率要求不高时才可以使用。
    忙循环实现起来很简单————在循环中不断旋转直到希望的时间节拍数耗尽。
###11.8.2 短延迟
    有时内核代码（通常也是驱动程序）不但需要很短暂的延迟（比时钟节拍还短），而且还要求延迟的时间很精确。这种情况多发生在和硬件的同步时，也就是说需要短暂等待某个动作的完成（往往小于1ms），而节拍间隔甚至会超过10ms（100HZ的时钟中断）那么怎么办满足更短、更精确的延迟要求？
    依靠执行数次循环达到延迟效果实现。因为内核知道处理器在1面内能执行多少次循环。BogoMIPS值纪律处理器在给定时间内忙循环执行的次数。可在/proc/cpuinfo中读到它。可用以计算小于1ms的精准延迟。大于1ms的不要使用这个，可造成溢出。
###11.8.3 schedule_timeout()
    更理想的延迟执行方法是使用schedule_timeout()函数，该方法会让需要延迟执行的任务睡眠到指定的延迟时间耗尽后再重新运行。但该方法也不能保证睡眠时间刚好等于指定的延迟时间，只能尽量使睡眠时间接近指定的延迟时间。当指定的时间到期后，内核唤醒被延迟的任务（推入可中断睡眠队列）并将其重新放回运行队列。
    2，设置超时时间，在等待队列上睡眠
    第四章我们已经看到进程上下文中的代码为了等待特定事件发生，可以将自己放入等待队列，然后调用导读程序去执行新任务。一旦事件发生后，内核调用wake_up()函数唤醒在睡眠队列上的任务，使其重新投入运行。有时，等待队列上的某个任务可能既在等待一个特定事件到来，又在等待一个特定时间到来（看谁快）。这时代码通过检查被唤醒的原因（事件、延迟时间到期、接收到信号），然后执行相应的操作。
##11.9 小结
    考察了时间的概念，知道了墙上时钟与计算机的正常运行时间如何管理。我们对比了相对时间和绝对时间以及绝对事件与周期事件。还覆盖了诸如时钟中断、时钟节拍、HZ以及jiffies等概念。
    
#第12章 内存管理
    内核中获取内存要比在用户空间复杂的多。在深入研究分配内存的接口之前，我们需要理解内核是如何管理内存的。
##12.1 页
    内核把物理页作为内存管理的基本单位。尽管处理器的最小可寻址单位通常为字（甚至字节），但是，内存管理单元（MMU，管理内存并把虚拟地址转换为物理地址的硬件）通常以页为单位进行处理。正因为如此，MMU以页（page）大小为单位来管理系统中的页表（这也是页表名的来由）。从虚拟内存的角度来看，页就是最小单位。
    page结构与物理页相关，而并非与虚拟页相关。因此，该结构对页的描述只是短暂的。即使页中所包含的数据继续存在，由于交换等原因，它们也可能不再和同一个page结构相关联。内核仅仅用这个page结构来描述当前时刻在相关的物理页中存放的东西。这种数据结构的目的在于描述物理内存本身，而不是描述包含在其中的数据。
    内核用这一结构来管理系统中的所有的页，因为内核需要知道一个页是否空闲（也就是页有没有被分配）。如果页已经被分配，内核还需要知道谁拥有这个页。拥有者可能是用户空间进程、动态分配的内核数据、静态内核代码或页高速缓存等。
   
    系统中的每个物理页都要分配一个这样的结构体。
##12.2 区
    由于硬件的限制，内核并不能对所有的页一视同仁。有些页位于内存中特定的物理地址上，所以不能将其用于一些特定的任务。由于存在这种限制，所以内核把页划分为不同的区（zone）。内核使用 区 对相似特性的页进行分组。
    Linux必须处理如下两种由于硬件缺陷而引起的内存寻址问题：
        1，一些硬件只能用某些特定的内存地址来执行DMA（直接内存访问）。
        2，一些体系结构的内存的物理寻址范围比虚拟寻址范围大得多。这样，就有一些内存不能永久地映射到内核空间上。
    因为存在这些制约条件，Linux主要使用了四种区：
        1，ZONE_DMA ———— 这个区包含的页能用来执行DMA操作。
        2，ZONE_DMA32 ———— 和ZONE_DMA相似，该区域包含的页面可用来执行DMA操作；而和ZONE_DMA不同之处在于，这些页面只能被32位设备访问。在某些体系结构中，该区将比ZONE_DMA更大。
        3，ZONE_NORMAL ———— 该区包含的都是能正常映射的页。
        4，ZONE_HIGHEM ———— 这个区包含"高端内存"，其中的页不能永久的映射到内核地址空间。
    Linux把系统的页划分为区，形成不同的内存池，这样就可以根据用途进行分配了。例如，ZONE_DMA内存池让内核有能力为DMA分配所需的内存。如果需要这样的内存，那么，内核就可以从ZONE_DMA中按照请求的数目取出页。注意，区的划分没有任何物理意思，这只不过是内核为了管理页而采取的一种逻辑上的分组。
##12.3 获得页
    我们已经对内核如何管理内存（页、区）有所了解，现在我们看一下内核实现的接口，我们正式通过这些接口在内核内分配和释放内存的。
    内核提供了一种请求内存的底层机制，并提供了对它进行访问的几个接口。所有这些接口都以页为单位分配内存，最核心和函数是：
        struct page * alloc_page(gfp_t gfp_mask, unsigned int order)
###12.3.1 获得填充为0的页
    把分配好的页都填充成了0————字节中的每一位都要取消设置。如果分配的页是给用户空间的，分配好的页中应该包含的是随机产生的垃圾信息，很可能有敏感信息。用户空间的页在返回之前，所有的数据必须填充为0，或做其他清理工作，从而保障系统安全。
###12.3.2 释放页（内核释放页）
    当不再需要页时可以通过下面函数释放它们，释放页时要谨慎，只能释放属于你的页。传递了错误的struct page 或地址，用了错误的order值，这些都可能导致系统崩溃。请记住，内核是完全依赖自己的。这点与用户空间不同，如果你有非法操作，内核会开开心心地把自己挂起来，停止运行。
##12.4 kmalloc()
    kmalloc()函数与用户空间的malloc（）一簇函数非常类似，用它可以获得以字节为单位的一块内核内存。如果你需要整个页，那么，前面讨论的页分配接口可能是更好的选择。分配成功后，返回一个指向内存块的指针，内存块的大小至少为所请求的大小。
###12.4.2 kfree()
    kfree()函数释放由kmalloc()分配出来的内存块。如果想要释放的内存不是由kmalloc()分配的，或者想要释放的内存早就被释放了，比如说释放属于内核其他部分的内存，调用这个函数会导致严重的后果。与用户空间类似，分配和回收要注意配对使用，以避免内存泄露和其他bug。注意，调用kfree是安全的。
##12.5 vmalloc()
    vmalloc()函数的工作方式类似于kmalloc()，只不过前者分配的内存虚拟地址是连续的，而物理地址则无须连续。这也是用户空间分配函数的工作方式：由malloc()返回的页在进程的虚拟地址空间内是连续的，但是，这并不保证他们在物理RAM（随机存取存储器，随机存取器 (random access memory)）中也是连续的。kmalloc()函数确保页在物理地址上是连续的（虚拟地址自然也是连续的）。vmalloc()函数只确保页在虚拟地址空间内是连续的。它通过分配非连续的物理内存块，再"修正"页表，把内存映射到逻辑地址空间的连续区域中，就能做到这点（内存管理单元（MMU，管理内存并把虚拟地址转换为物理地址的硬件）再进行映射，不连续没关系）。
    大多数情况下，只有硬件设备需要得到物理地址连续的内存。在很多体系结构上，硬件设备存在于内存管理单元（MMU，管理内存并把虚拟地址转化为物理地址的硬件）以外，它根本不理解什么是虚拟地址。因此，硬件设备用到的任何内存区都必须是物理上连续的块，而不仅仅是虚拟地址连续上的块。而仅供软件使用的内存块（例如与进程相关的缓冲区）就可以使用只有虚拟地址连续的内存块。但在你的编程中，根本察觉不到这种差异。对内核而言，所有内存看起来都是逻辑上连续的。
    尽管在某些情况下需要物理上连续的内存块，但是，很多内核代码都用kmalloc()来获得内存，而不是vmalloc()。这主要是出于性能的考虑。vmalloc（）函数为了把物理上不连续的页转换为虚拟地址空间上连续的页，必须专门建立页表项。糟糕的是，通过vmalloc()获得的页必须一个一个地进行映射（因为它们物理上是不连续的），这就会导致比直接内存映射大得多的TLB（translation lookaside buffer 是一种硬缓冲区，用来缓存虚拟地址到物理地址的映射关系。极大地提高了系统的性能，因为大多数内存都要进行虚拟寻址）抖动。因为这些原因，vmalloc()仅在不得已时才会使用————典型就是为了获得大块内存时，例如，当模块被动态插入到内核中时，就把模块装载到由vmalloc()分配的内存上。
##12.6 slab层
    分配和释放数据结构是所有内核中最普遍的操作之一。为了便于数据的频繁分配和回收，编程人员常常会用到空闲链表。空闲链表包含可供使用的、已经分配好的数据结构块。当代码需要一个新的数据结构实例时，就可以从空闲链表中抓取一个，而不需要分配内存，再把数据放进去。以后，当不再需要这个数据结构的实例时，就把它放回空闲链表，而不是释放它。从这个意义上说，空闲链表相当于对象高速缓存————快速存储频繁使用的对象类型。
    当内核程序使用时，内核根部就不知道存在任何空闲链表。为了使代码更加稳固，Linux内核提供了slab层（slab分配器）。slab分配器扮演了通用数据结构缓存层的角色。
    slab分配器的基本原则：
        1，频繁使用的数据结构也会频繁分配和释放，因此应当缓存它们。
        2，频繁分配和回收必然会导致内存碎片（难以找到大块连续的可用内存）。为了避免这种现象，空闲链表的缓存会连续地存放。因为已释放的数据结构又会放回空闲链表，因此不会导致碎片。
        3，回收的对象可以立即投入下一次分配，因此，对于频繁的分配和释放，空闲链表能够提高其性能。
        4，如果分配器知道对象大小、页大小和总的高速缓存的大小这样的概念，它会做出更明智的决策。
        5，如果让部分缓存专属于单个处理器（对系统上的每个处理器独立而唯一），那么，分配和释放就可以在不加SMP（对称多处理器）锁的情况下进行。
        6，如果分配器是与NUMA（非均衡存储访问）相关的，它就可以从相同的内存节点为请求者进行分配。
        7，对存放的对象进行着色（color），以防止多个对象映射到相同的高速缓存行（cache line）。
###12.6.1 slab层的设计
    slab层把不同的对象划分为所谓高速缓存组，其中每个高速缓存组都存放不同类型的对象。每种对象类型对应一个高速缓存。例如，一个高速缓存用于存放进程描述符（task_struct结构的一个空闲链表），而另一个高速缓存存放索引节点对象（struct inode)。有趣的是，kmalloc()接口建立在slab层之上，使用了一组通用高速缓存。
    然而，这些高速缓存又被划分为slab（这也是这个子系统名字的来由）。slab由一个或多个物理上连续的页组成。一般情况下，slab也就仅仅由一页组成。每个高速缓存可以由多个slab组成。
    slab层负责内存紧缺情况下所有底层的对其、着色、分配、释放和回收等。如果你要频繁创建很多相同类型的对象，那么，就应该考虑使用slab高速缓存。也就是说，不要自己去实现空闲链表！
##12.7 在栈上静态分配
    在用户空间，我们以前讨论过到的那些分配的例子，有不少都可以在栈上发生。因为我们可以事先知道所分配空间的大小。用户空间能够奢侈地负担起非常大的栈，而且栈空间还可以动态增长，相反，内核却不能这么奢侈————内核栈小而且固定。当给每个进程分配一个固定大小的栈后，不但可以减少内存的消耗，而且内核也无须负担太重的栈管理任务。
    历史上，每个进程都有两个页的内核栈。因为32位和64位体系结构的页面大小分别是4KB和8KB，所以通常它们的内核栈的大小分别是8KB和16KB。
###12.7.1 单页内核栈
    引入了一个选项设置单页内核栈。激活这个选项时，每个进程的内核栈只有一页那么大。这么做出于两个原因：
        1，减少内存消耗
        2，最重要的原因，随着机器运行时间的增加，寻找两个未分配的、连续的页变得越来越困难。物理内存渐渐变为碎片，因此，给一个新进程分配虚拟内存（VM）的压力也在增大。
        3，复杂的原因，每个进程的整个调用链必须放在自己的内核栈中。不过，中断处理程序也曾经使用它们所中断的进程的内核栈，这样，中断处理程序也要放在内核栈中。这当然简单有效，但是，同样会把更严格的约束条件放在这可怜的内核栈上。当我们使用只有一个页面的内核栈时，中断处理程序放在 中断栈中了（矫正上述问题开发的新功能）。中断栈为每个进程提供一个用于中断处理程序的栈。
###12.7.2 在栈上光明正大的工作
    在任意一个函数中，你都必须尽量节省栈资源。做法是在具体的函数中让所有局部变量所占空间之和不要超过几百字节。在栈上进行大量的静态分配（比如分配大型数组或大型结构体）是很危险的。要不然，哎内核中和在用户控件中进行的栈分配就没有什么差别了。
##12.8 高端内存的映射
    根据定义，在高端内存中的页不能永久的映射到内核地址空间上。
###12.8.1 永久映射 （进程上下文）
    永久的映射一个给定的page结构到内核地址空间。允许永久映射的page数量是优先的，当不再需要高端内存是，应该解除映射。
###112.8.2 临时映射（中断上下文）
    当必须创建一个映射而当前的上下文又不能睡眠时，内核提供了临时映射（原子映射）。
##12.9 每个CPU的分配
    支持SMP的现代操作系统使用每个CPU上的数据，对于给定的处理器其数据是唯一的。一般来说，每个CPU的数据存放在一个数组中。数组中的每一项对应着系统上一个存在的处理器。按当前处理器号确定这个数组的当前元素。
##12.10 新的每个CPU接口
##12.11 使用每个CPU数据的原因
    使用每个CPU数据具有不少好处。
        首先是减少了数据锁定。因为按照每个处理器访问每个CPU数据的逻辑，你可以不再需要任何锁。
        第二个好处是使用每个CPU数据可以大大减少缓存失效。失效发生在处理器试图使它们的缓存保持同步时。如果一个处理器操作某个数据，而该数据又存放在其他处理器缓存中，那么存放该数据的哪个处理器必须清理或刷新自己的缓存。持续不断的缓存失效称为缓存抖动，这样对系统性能影响颇大。使用每个CPU数据将使得缓存影响降至最低，因为理想情况下只会访问自己的数据。
    使用每个CPU数据会省去许多数据上锁，它唯一的安全要求就是要禁止内核抢占。而这点代价相比上锁要小得多。但是要注意，不能在访问每个CPU数据过程中睡眠————否则，你就可能醒来后已经到其他处理器上了。
##12.12 分配函数的选择
    kmalloc(): 如果你需要连续的物理页
    allow_pages(): 高端内存分配
    vmalloc(): 如果不需要物理上连续的页，仅仅需要虚拟地址连续的页。
    slab高速缓存：如果你要创建和撤销很大的数据结构。
##12.13 小结
    本章中，学习了Linux内核如何管理内存。首先看到了内存空间的各种不同的描述单位，包括字节、页面和区。接着讨论了各种内存分配机制，其中包括页分配器和slab分配器。在内核中分配内存需要小心地确保分配过程遵从内核特定的状态约束。比如分配过程中不得阻塞，或者访问文件系统等约束。为此我们讨论了gfp标识以及使用每个标识的针对场景。

#第13章 虚拟文件系统
    虚拟文件系统（有时也称作虚拟文件交换，更常见的是简称VFS）作为内核子系统，为用户空间程序提供了文件和文件系统相关的接口。系统中所有文件系统不但依赖VFS共存，而且也依靠VFS系统协同工作。通过虚拟文件系统，程序可以利用标准的Unix系统调用不同的文件系统，甚至不同介质的文件系统进行读写操作。
##13.1 通用文件系统接口
    VFS使得用户可以直接使用open()、read()和write()这样的系统调用而无须考虑具体文件系统和实际物理介质。由于现代操作系统引入抽象层，比如Linux，通过虚拟接口访问文件系统，才使得这种协作和泛型存取成为可能。VFS与块I/O层结合，提供抽象、接口以及交融，使得用户空间额程序调用统一的系统调用访问各种文件，不管文件系统是什么，也不管文件系统位于何种介质，采用的命名策略是统一的。
##13.2 文件系统抽象层
    底层文件系统之上建立了一个抽象层，使Linux能够支持各种文件系统，即便它们在功能和行为上存在很大差别。为了支持多文件系统，VFS提供了一个通用文件系统模型，该模型囊括了任何文件系统的常用功能集和行为。
    文件系统的内部细节都被隐藏了。比如一个简单的用户空间程序执行如下操作：
    ret = write(fd, buf, len);
    该系统调用将buf指针指向的长度为len字节的数据写入文件描述符fd对应的文件的当前位置。这个系统调用首先被一个通用系统调用sys_write(VFS虚拟文件系统)处理，sys_write()函数要找到fd所在的文件系统实际给出的是哪个写操作，然后再执行该操作。实际文件系统的写方法是文件系统实现的一部分，数据最终通过该操作写入介质（或执行这个文件系统想要完成的写动作）。
##13.3 Unix文件系统
    Unix使用了四种和文件系统相关的传统抽象概念：文件、目录项、索引节点和安装点（mount point）。
    从本质上讲文件系统是特殊的数据分层存储结构，它包含文件、目录和相关的控制信息。文件系统的通用操作包含：创建、删除和安装等。在Unix中，文件系统被安装在一个特定的安装点上，该安装点在全局层次结构中被称作命名空间，所有的已安装文件系统都作为根文件系统树的枝叶出现在系统中。与这种单一、统一的树形形成鲜明对照的就是DOS和WINDOWs的表现，它们将文件的命名空间分类为驱动字母，例如C:。这种将命名空间划分为设备和分区的做法，相当于把硬件细节"泄露"给了文件系统抽象层。
    文件其实可以做一个有序字符串，字符串中第一个字节是文件的头，最后一个字节是文件的尾。
    文件通过目录组织起来。文件目录还比一个文件夹，用来容纳相关文件。因为目录也可以包含其他目录，即子目录，所以目录可以层层嵌套，形成文件路径。路径中的每一部分被称作目录条目。在Unix中，目录属于普通文件，它列出包含在其中的所有文件。由于VFS把目录当做文件对待，所以可以对目录执行和文件相同的操作。
    Unix系统将文件的相关信息和文件本身这两个概念加以区分，例如访问控制权限、大小、拥有者、创建时间等信息。文件相关信息，有时被称作文件的元数据（也就是说，文件的相关数据），被存储再一个单独的数据结构中，被称为索引节点inode（index node）。
##13.4 VFS对象及其数据结构
    VFS中有四个主要的对象类型，分别是：
        超级块对象，它代表一个具体的已安装文件系统。
        索引节点对象，它代表一个具体文件。
        目录项对象，它代表一个目录项，是路径的一个组成部分。
        文件对象，它代表由进程打开的文件。
    注意，因为VFS将目录作为一个文件来处理，所以不存在目录对象。换句话说，目录项不同于目录，但目录是另一种形式的文件，明白了吗？
    每个主要对象都包含一个操作对象（对主要对象封装了其操作，用以调用），这些操作对象描述了内核针对主要对象可以使用的方法：
        super_operations对象，其中包括内核针对特定文件系统所能调用的方法，比如write_inode()和sync_fs()等方法。
        inode_operations对象，其中包括内核针对特定文件所能调用的方法，比如create()和link()等方法。
        dentry_operations对象，其中包括内核针对特定目录所能调用的方法，比如d_compare()和d_delete()等方法。
        file_operations对象，其中包括进程针对已打开文件所能调用的方法，比如read()和write()等方法。
    操作对象作为一个结构体指针来实现，此结构体中包含指向操作其父对象的函数指针。对于其中许多方法来说，可以继承使用VFS提供的通用函数，如果通用函数提供的基本功能无法满足要求，那么久必须使用实际文件系统的独有方法填充这些函数指针，使其指向文件系统实例。
##13.5 超级块对象 super_block
    各种文件系统都必须实现超级块对象，该对象用于存储特定文件系统的信息，通常对应与存放在磁盘特定扇区中的文件系统超级块或文件系统控制块（所以称为超级块对象），其结构体是 super_block。通过alloc_super()函数创建并初始化。
##13.6 超级块操作 super_operations
    超级块对象中最重要的一个域是s_op，它指向超级块的操作函数表。其结构体是 super_operations。该结构体重的每一项都是一个指向超级块操作函数的指针，超级块操作函数执行文件系统和索引节点的底层操作。
##13.7 索引节点对象 inode
    索引节点对象包含了内核在操作文件或目录时需要的全部信息。一个索引节点代表文件系统中（但是索引节点仅当文件被访问时，才在内存中创建）的一个文件，它也可以是设备或管道这样的特殊文件。因此索引节点结构体中有一些和特殊文件相关的项，比如i_pipe项就指向一个代表有名管道的数据结构，i_bdev指向块设备结构体（块设备驱动），i_cdev指向字符设备结构体。这三个指针被存放在一个共用体（union）中，因为一个给定的索引节点每次只能表示三者之一（或三者均不）。
##13.8 索引节点操作 inode_operations
    和超级块操作一样，索引节点对象中的inode_operations项也非常重要，因为它描述了VFS用以操作索引节点对象的所有方法，这些方法由文件系统实现。
##13.9 目录项对象 dentry
    VFS把目录当文件对待，所以在路径/bin/vi.txt中，bin和vi都属于文件————bin是特殊的目录文件而vi是一个普通文件，路径中的每个组成部分都由一个索引节点对象表示。虽然它们可以统一由索引节点表示，但是VFS经常需要执行目录相关的操作，比如路径名查找等。路径名查找需要解析路径中的每一个组成部分，不但要确保它有效，而且还需要再进一步寻找路径中的下一个部分。
    为了方便查找操作，VFS引入了目录项的概念。每个dentry代表路径中的一个特定部分。对前一个例子来说，/、bin、vi都属于目录项对象。前两个是目录，最后一个是普通文件。必须明确一点：在路径中（包括普通文件在内），每一个部分都是目录项对象。解析一个路径并遍历其分量绝非简单的演练，它是耗时的，常规的字符串比较过程，执行耗时，代码繁琐。目录项对象的引入使得这个过程更加简单。
    与前面两个对象不同（超级块对象和索引节点对象），目录项对象没有对应的磁盘数据结构，VFS根据字符串形式的路径名现场创建它。而且由于目录项对象并非真正保存在磁盘上，所以目录项结构体没有是否被修改的标志（也就是是否为脏、是否需要写回磁盘的标志）。
###13.9.1 目录项状态
    目录项对象有三种有效状态：被使用、未被使用和负状态。
###13.9.2 目录项缓存 d_cache
    如果VFS层遍历路径名中所有的元素并将它们逐个地解析成目录项对象，还要到达最深层目录，将是一件非常费力的工作，会浪费大量的时间。所以内核将目录项对象缓存在目录项缓存（d_cache）中。
    目录项缓存包括三个主要部分：
        "被使用的"目录项链表。"最近被使用的"双向链表。"散列表和相应的散列函数用来快速地将给定路径解析为相关目录项对象"。
##13.10 目录项操作 dentry_operations
##13.11 文件对象 struct file
    VFS的最后一个主要对象是文件对象。文件对象表示进程已打开的文件。如果我们站在用户角度来看待VFS，文件对象会首先进入我们的视野。进程直接处理的是文件，而不是超级块、索引节点或目录项。所以不必奇怪：文件对象包含我们非常熟悉的信息（如访问模式，当前偏移等），同样道理，文件操作和我们非常熟悉的系统调用read()和write()等也很类似。
    文件对象是已打开的文件在内存中的表示。该对象（不是物理文件）由相应的open()系统调用创建，由close()系统调用撤销，所有这些文件相关的调用都是文件操作表中定义的方法。因为多个进程可以同时打开和操作同一个文件，它反过来指向目录项对象（反过来指向索引节点），其实只有目录项对象才表示已打开的实际文件。虽然一个文件对应的文件对象不是唯一的，但对应的索引节点和目录项对象无疑是唯一的。
    类似于目录项对象，文件对象实际上没有对应的磁盘数据。所以在结构体中没有代表其对象是否为脏、是否需要写回磁盘的标志。文件对象通过f_dentry指针指向相关的目录项对象。目录项指向相关的索引节点，索引节点会记录文件是否是脏的。
##13.12 文件操作 file_operations
##13.13 和文件系统相关的数据结构 file_system_type
    除了以上几种VFS基础对象外，内核还使用了另外一些标准数据结构来管理文件系统的其他相关数据。第一个对象是file_system_type，用来描述各种特定文件系统类型，比如ext3、ext4或UDF。第二个结构体是vfsmount，用来描述一个安装文件系统的实例。
    file_system_type：因为Linux支持众多的文件系统，所以内核必须由一个特殊的结构来描述每种文件系统的功能和行为。
    vfsmount：当文件系统被实际安装时，将有一个vfsmount结构体在安装点被创建。该结构体用来代表文件系统的实例————一个安装点。
##13.14 和进程相关的数据结构
    系统中的每一个进程都有自己的一组打开的文件，像根文件系统、当前工作目录、安装点等。有三个数据结构将VFS层和系统的进程紧密联系在一起，它们分别是：file_struct、fs_struct和namespace结构体。
    file_struct：该结构体由进程描述符中的files目录项指向（是task_struct中的一个属性）。所以单个进程（per-process)相关的信息（如打开的文件及文件描述符）都包含在其中。
    fs_struct：该结构体由进程描述符的fs域指向。它包含文件系统和进程相关信息。
    namespace：由进程描述符mmt_namespace域指向。
##13.15 小结
    Linux支持了相当多种类的文件系统。从本地文件系统（如ext3和ext4）到网络文件系统（如NFS和Coda），Linux在标准内核中已支持的文件系统超过60种。VFS层提供给这些不同文件系统一个统一的实现框架，而且也提供了能和标准系统调用交互工作的统一接口。由于VFS层的存在，使得在Linux上实现新文件系统的工作变得简单起来，它可以轻松地使这些文件系统通过标准Unix系统调用而协同工作。
    
#第14章 块I/O层
    系统中能够随机（不需要按顺序）访问固定大小数据片（chunks）的硬件设备称作块设备，这些固定大小的数据片就称作块。最常见的块设备是硬盘，除此之外，还有软盘驱动器、蓝光光驱和闪存等许多其他设备。注意，它们都是以安装文件系统的方式使用的————这也是块设备一般的访问方式。
    另一种基本的设备类型是字符设备。字符设备按照字符流的方式被有序访问（块设备是随机访问），像串口和键盘就属于字符设备。如果一个硬件设备是以字符流的方式被访问的话，那就应该将它归于字符设备；反过来，如果一个设备是随机（无序的）访问的，那么它就属于块设备。
    对于这两种类型的设备，它们的区别在于是否可以随机访问数据————换句话说，就是能否在访问设备时随意地从一个位置跳转到另一个位置。举个例子，键盘这种设备提供的就是一个数据流，当你输入"wolf"这个字符串时，键盘驱动程序会按照和输入完全相同的顺序返回这个由四个字符组成的数据流。如果让键盘驱动程序打乱顺序来读字符串，或读取其他字符，都是没有意义的。所以键盘就是一种典型的字符设备，它提供就是用户从键盘输入的字符流。对键盘进行读操作就会的到一个字符流，首先是"w"，然后是"o"，再是"l"，最后是"f"。当没人敲键盘时，字符流就是空的。硬盘设备的情况就不大一样了。硬盘设备的驱动可能要求读取磁盘上任意块的内容，然后又转去读取别的块的内容，而被读取的块在磁盘上位置不一定要连续。所以说硬盘的数据可以被随机访问，而不是以流的方式被访问，因此它是一个块设备。
    内核管理块设备要比管理字符设备细致得多，需要考虑的问题和完成的工作相对于字符设备来说要复杂许多。这是因为字符设备仅仅需要控制一个位置————当前位子，而块设备访问的位置必须能够在介质的不同区间前后移动。所以事实上内核不必提供一个专门的子系统来管理字符设备，但是对块设备的管理却必须要有一个专门的提供服务的子系统。不仅仅是因为块设备的复杂性远远高于字符设备，更重要的原因是块设备对执行性能的要求很高：对硬盘每多一分利用都会对整个系统的性能带来提升，其效果要远远比键盘吞吐速度澄碧的提高大很多。另外，我们将会看到，块设备的复杂性会为这种优化留下很大的施展空间。这一章的主题就是讨论内核如何对块设备和块设备的请求进行管理。该部分在内核中称作块I/O层。
##14.1 剖析一个块设备
    块设备（硬件）中最小的可寻址单元是扇区。扇区大小一般是2的整数倍，而最常见的是512字节。扇区的大小是设备的物理属性，扇区是所有块设备的基本单元————块设备无法对比它还小的单元进行寻址和操作，尽管许多块设备能够一次对多个扇区进行操作。
    软件用的最小逻辑可寻址单元（注意是逻辑，逻辑就有映射）————块。块是文件系统的一种抽象————只能基于块来访问文件系统。虽然物理磁盘寻址是按照扇区及进行的，但是内核执行的所有磁盘操作都是按照块进行的。由于扇区是设备的最小可寻址单元，所以块不能比扇区还小，只能数倍于扇区的大小。另外，内核（对有扇区的硬件设备）还要求块大小是2的整数倍，而且不能超过一个页(内核把物理页作为内存管理的基本单位）的长度。所以对块的最终要求是，必须是扇区大小的2的整数倍，并且要小于页面大小。随意块大小是512字节、1KB或4KB。
    其他叫法：
        扇区————设备的最小寻址单元，有时会称作"硬扇区"或"设备块"；扇区这一术语之所以对内核重要，是因为所有设备的I/O必须以扇区为单元进行操作。
        块————文件系统的最小寻址单元，有时会乘坐"文件块"或"I/O"块。内核所使用的"块"这一高级概念就是建立在扇区之上的。
##14.2 缓冲区和缓冲区头
    当一个块被调度内存时（也就是说，在读入后或等待写出时），它要存储在一个缓冲区中。每个缓冲区与一个块对应，它相当于是磁盘块在内存中的表示。前面提高过，块包含一个或多个扇区，但大小不能超过一个页面，所以一个页可以容纳一个或多个内存中的块。由于内核在处理数据时需要一些相关的控制信息（比如块属于哪一个块设备，块对应于哪个缓冲区等），所以每一个缓冲区都有一个对应的描述符。该描述符用buffer_head结构体表示，称作缓冲区头。
    2.6版本内核以前，缓冲区头作为内核中的I/O操作单元，不仅仅描述了从磁盘块到物理内存的映射，而且还是所有块I/O操作的容器。将缓冲区头作为I/O操作的两个弊端：
        1，对内核来说，它更倾向于操作页面结构，因为页面操作起来更简便，效率高。使用一个巨大的缓冲区头表示每一个独立的缓冲区（比页小）效率低下。
        2，它仅能描述单个缓冲区，当作为所有I/O的容器使用时，缓冲区头会促使内核把对大块数据的I/O操作（比如写操作）分解为对多个buffer_head结构体进行操作。这样做必然造成不必要的负担和空间浪费（本来可以批处理的）。2.5版本的内核为块I/O操作引入了一种新型、灵活并且轻量级的容器，bio结构体。
##14.3 bio结构体
    目前内核中块I/O操作的基本容器由bio结构体表示。该结构体代表了正在现场（活动的）以片段（segment）链表形式组织的块I/O操作。一个片段是一小块连续的内存缓冲区。这样的话，就不需要保证单个缓冲区一定要连续。所以通过用片段描述缓冲区，即使一个缓冲区分在在内存的多个位置上，bio结构体也能对内核保证I/O操作的执行。像这样的向量I/O就是所谓的聚散I/O。
###14.3.1 I/O向量
    bi_io_vec域指向一个bio_vec结构体数组，该结构体链表包含了一个特定I/O操作所需要使用到的所有片段。每个bio_vec结构都是一个形式为<page（指向这个缓冲区所驻留的物理页）,offset（缓冲区所驻留的页中以字节为单位的偏移量）,len（这个缓冲区以字节为单位的大小）>的向量，它描述的是一个特定的片段（segment）：片段所在的物理页、块在物理页中的偏移位置、从给定偏移量开始的块长度。
    总而言之，每一个块I/O请求都通过一个bio结构体表示。每个请求包含一个或多个块，这些块存储再bio_vec结构体数组中。这些结构体描述了每个片段在物理页中的实际位置，并且像向量一样被组织在一起。I/O操作的第一个片段由b_io_vec结构体锁指向，其他的片段在气候依次放置，共有bi_vent个片段。当块I/O层开始执行请求、需要使用各个片段时，bi_idx域会不断更新，从而总指向当前片段。
    bi_idx域指向数组中的当前bio_vec片段，块I/O层通过它可以跟踪块I/O操作的完成进度。但该域更重要的作用在于分割bio结构体。像冗余廉价磁盘阵列（RAID，出于提高性能和可靠性的目的，将单个磁盘的卷扩展到多个磁盘上）这样的驱动器可以把单独的bio结构体（原本是为单个设备使用准备的），分割到RAID阵列中的各个硬盘上去。RAID设备驱动只需要拷贝这个bio结构体，再把bi_idx域设置为每个独立硬盘操作时需要的位置就可以了。
###14.3.2 新老方法对比
    缓冲区头和新的bio结构体之间存在显著差别。bio结构体代表的是I/O操作。它可以包括内存中的一个或多个页；而另一方面，buffer_head结构体代表的是一个缓冲区，它描述的仅仅是磁盘中的一个块。因为缓冲区头关联的是单独页中的单独磁盘块，所以它可能会引起不必要的分割，将请求按块为单位划分，只能靠以后才能再重新组合。由于bio结构体是轻量级的，它描述的块可以不需要连续存储区，并且不需要分割I/O操作。
    bio 比 buffer_head 结构体的好处：
        1，bio结构体很容易处理高端内存，因为它处理的是物理页而不是直接指针。
        2，bio结构体既可以代表普通页I/O，同时也可以代表直接I/O（指那些不通过页高速缓存的I/O操作，详见第16章）。
        3，bio结构体便于执行分散——集中（矢量化的）块I/O操作，操作中的数据可取自多个物理页面。
        4，bio结构体相比缓冲区头属于轻量级的结构体。因为它只需要包含块I/O操作所需的信息就可以了，不用包含与缓冲区本身相关的不必要信息。
    但还是需要缓冲区头这个概念，毕竟它还负责描述磁盘块到页面的映射。bio结构体不包含任何和缓冲区相关的状态信息————它仅仅是一个矢量数组，描述一个或多个单独块I/O操作的数据片段和相关信息。
##14.4 请求队列 request_queue
    块设备将它们挂起的"块I/O请求"保存在请求队列中，包含一个双向请求链表以及相关控制信息。通过内核中像文件系统这样高层的代码将请求加入到队列中。请求队列只要不为空，队列对应的块设备驱动程序（看起来貌似多个块设备对应多个队列）就会从队列头获取请求，然后将其送入对应的块设备上去。请求队列表中的每一项都是一个单独的请求，由request结构体表示。因为一个请求可能要操作多个连续的磁盘块，所以每个请求可以由多个bio结构体组成。注意，虽然磁盘上的块必须连续，但是在内存中这些块并不一定要连续————每个bio结构体都可以描述多个片段（片段是内存中连续的小区域），而每个请求也可以包含多个bio结构体。
##14.5 I/O调度程序
    如果简单地以内核产生请求的次序直接将请求发向块设备的话，性能肯定让人难以接受。磁盘寻址是整个计算机中最慢的操作之一，每一次寻址（定位硬盘磁头到特定块上的某个位置）需要花费不少时间。所以尽量缩短寻址时间无疑是提高系统性能的关键。
    为了优化寻址操作，内核既不会简单地按请求接收次序，也不会立即将其提交给磁盘。相反，它会在提交前，先执行名为合并与排序的预操作，这种预操作可以极大地提高系统的整体性能（前提条件是预操作实现的好）。在内核中负责提交I/O请求的子系统称为I/O调度程序。
    I/O调度程序将磁盘I/O资源分配给系统中所有挂起的块I/O请求。具体地说，这种资源分配是通过将请求队列中挂起的请求合并和排序来完成的。注意不要将I/O调度程序和进程调度程序混淆。进程调度程序的作用是将处理器资源分配给系统中的运行进程。这两种子系统看起来非常相似，但并不相同。进程调度成和和I/O调度程序都是将一个资源虚拟给多个对象，对进程调度程序来说，处理器被虚拟冰杯系统中的运行进程共享。这种虚拟提供给用户的就是多任务和分时操作系统，像Unix系统。相反，I/O调度程序虚拟块设备给多个清盘请求，以便降低磁盘寻址时间，确保磁盘性能的最优化。
###14.5.1 I/O调度程序的工作
    I/O调度程序的工作是管理块设备的请求队列。它决定队列中的请求排列顺序以及在什么时刻派发请求到块设备。这样做有利于减少磁盘寻址时间，从而提高全局吞吐量。
    I/O调度程序通过两种方法减少磁盘寻址时间：合并与排序。合并指将两个或多个请求结合成一个新请求。考虑一下这种情况，文件系统提交请求到请求队列————从文件中读取一个数据区（当然，最终所有的操作都是针对扇区和块进行的，而不是文件，还假定请求的块都是来自文件块），如果这时队列中已经存在一个请求，它访问的磁盘扇区和当前请求访问的磁盘扇区相邻（比如，同一个文件中早些时候被读取的数据区），那么这两个请求就可以合并为一个对单个和多个相邻磁盘扇区操作的新请求（两个请求变一个请求，此为合并）。通过合并请求，I/O调度程序将多次请求的开销压缩成一次请求的开销。更重要的是，请求合并后只需要传递给磁盘一条寻址命令，就可以访问到请求合并前必须多次寻址才能访问完的磁盘区域了，因此合并请求显然能减少系统开销和磁盘寻址次数。
    现在，假设在读请求被提交给请求队列的时候，队列中并不需要操作相邻扇区的其他请求，此时就无法将当前请求与其他请求合并，当然，可以将其插入请求队列的尾部。但是如果有其他请求需要操作磁盘上类似的位置呢？如果存在一个请求，它需要操作的磁盘扇区位置与当前请求比较接近，那么是不是该让这两个请求再请求队列上也相邻呢？事实上，I/O调度程序的确是这样处理的，整个请求队列将按扇区增长方向有序排列。使所有请求按磁盘上扇区的排列顺序有序排列，更重要的优化在于，通过保持磁头以直线方向移动，缩短了所有请求的寻址时间。该排序算法类似于电梯调度————电梯不能随意地从一层跳到另一层，向一个方向移动，当抵达了同一方向上的最后一层厚，再掉头向另一个方向移动。由于这种相似性，所以I/O调度程序称作电梯调度。
###14.5.2 Linus电梯
    Linus电梯I/O调度程序可以执行向前和向后的合并。鉴于文件的分布（通常以扇区号的增长表现）特点和I/O操作执行方式具有典型性（一般都是从头到尾，很少反方向读）。所以向前合并相比向后合并要少得多。
    如果合并尝试失败，那么就需要寻找可能的插入点（新请求在队列中的位置必须符合请求以扇区方向有序排序的原则）。
    简言之，当一个请求加入到队列中时，有可能发生四种操作：
        1，如果队列中已经存在一个对相邻磁盘扇区操作的请求，那么新请求将和这个已经存在的请求合并成一个请求。
        2，如果队列中存在一个驻留时间过长的请求，那么新请求将被插入到队列尾部，以防止其他旧的请求饥饿发生。（不幸的是，这种"年龄"检查方法并不是很有效，因为它并非是给等待了一段时间的请求提供实质性服务，它仅仅是在经过了一定时间后停止插入————排序请求，这改善了等待时间但最终还是会导致请求饥饿现象发生（就后面来看，貌似是容易过期、超时））
        3，如果队列中以扇区方向为序存在合适的插入位置，那么新的请求将被插入到该位置，保证队列中的请求是以被访问磁盘物理位置为序进行排序的。
        4，如果队列中不存在合适的请求插入位置，请求将被插入到队列尾部。
###14.5.3 最终期限I/O调度程序
    最终期限（deadline）I/O调度程序是为了解决Linus电梯带来的饥饿问题而提出的。出于减少磁盘寻址时间的考虑，对某个磁盘区域上的繁重操作，无疑会使得磁盘其他位置上的操作请求得不到运行机会。
    
#第15章 进程地址空间
    内核除了管理本身的内存物理外，还必须管理用户空间中进程的内存。我们称这个内存为进程地址空间，也就是系统中每个用户空间进程所看到的内存。Linux操作系统采用虚拟内存技术，因此，系统中的所有进程之间以虚拟方式共享内存。对一个进程而言，它好像都可以访问整个系统的所有物理内存。更重要的是，即使单独一个进程，它拥有的地址空间也可以远远大于系统物理内存。本章讨论内核如何管理进程地址空间。
##15.1 地址空间 (虚拟内存)
    进程地址空间由进程可寻址的虚拟内存组成，而且更为重要的特点是内核允许进程使用这种虚拟内存中的地址。每个进程都有一个32位或64位的平坦（flat）地址空间，空间的具体大小取决于体系结构。术语"平坦"指的是地址空间范围是一个独立的连续区间（比如，地址从0扩展到4294967295的32位地址空间）。一些操作系统提供了段地址空间，这种地址空间并非是一个独立的线性区域，而是被分段的，但现代采用虚拟内存的操作系统通常都使用平坦地址空间而不是分段式的内存模式。通常情况下，每个进程都有唯一的这种平坦地址空间。一个进程的地址空间与另一个进程的地址空间即使有相同的（物理）内存地址，实际上也彼此互不相干。我们称这样的进程为线程。
    内存地址是一个给定的值，它要在地址空间范围之内，比如4021f000，这个值表示的是进程32位地址空间中的一个特定的字节（地址空间和值）。尽管一个进程可以寻址4GB的虚拟内存（在32位的地址空间中），但这并不代表它就有权访问所有的虚拟地址。在地址空间中，我们更关系的是一些虚拟内存的地址空间，比如08048000-0804c000，它们可能被进程访问。这些可被访问的合法地址空间称为内存区域（memory areas）。通过内核，进程可以给自己的地址空间动态地添加或减少内存区域。
    进程只能访问有效内存区域内的内存地址。每个内存区域也具有相关权限如对相关进程有可读、可写、可执行属性。如果一个进程访问了不在有效范围中的内存区域，或以不正确的方式访问了有效地址，那么内核就会终止该进程，并返回"段错误"信息。
    （物理）内存区域可以包含各种内存对象，比如：
        1，可执行文件`代码的内存映射`，称为代码段（text section）。
        2，可执行文件的已初始化全局`变量的内存映射`，称为数据段（data section）。
        3，包含未初始化全局变量，也就是bss段的零页（页面中的信息全部为0值，所以可用于映射bss段等目的）的内存映射。（block started by symbol。因为未初始化的变量没有对应的值，所以并不需要存放在可执行对象中。但是C标准强制规定未初始化的全局变量要被赋予特殊的默认值（基本上是0），所以内核要将变量（为负值的）从可执行代码载入到内存中，然后将零页映射到该片内存上，于是这些未初始化变量就被赋予了0值。这样做避免了在目标文件中显示地进行初始化，减少了空间浪费）。
        4，用于进程用户空间栈（不要和进程内核栈混淆，进程的内核栈独立存在并由内核维护）的零页的内存映射。
        5，每一个诸如C库或动态连接程序等共享库的代码段、数据段和bss也会被载入进程的地址空间。
        6，任何内存映射文件。
        7，任何共享内存段。
        8，任何匿名的内存映射，比如由malloc() 分配的内存。
    进程地址空间中的任何有效地址都只能位于唯一的区域，这些内存区域不能相互覆盖。可以看到，在执行的进程中，每个不同的内存片段都对应一个独立的内存区域：栈、对象代码、全局变量、被映射的文件等。
##15.2 内存描述符（mm_struct映射了vma虚拟内存区域和 物理内存页）
    内核使用内存描述符结构体表示进程的地址空间，该结构包含了和进程地址空间有关的全部信息。内存描述符由mm_struct结构体表示。具体结构看书。
###15.2.1 分配内存描述符
    在进程的进程描述符（task_struct)中，mm域存放着该进程使用的内存描述符，所以current-> mm便指向当前进程的内存描述符。fork()函数利用copy_mm()函数复制父进程的内存描述符给其子进程，而子进程中的mm_struct结构体实际是通过文件fork.c中的allocate_mm()宏从mm_cachep slab缓存中分配得到的。通常，每个进程都有唯一的mm_struct结构体，即唯一的进程地址空间。
    如果父进程希望和其子进程共享地址空间，可以再调用clone()时，设置CLONE_VM标志。我们把这样的进程称作线程。回忆第3章，是否共享地址空间几乎是进程和Linux中所谓的线程间本质上的唯一区别。
    当CLONE_VM被指定后，内核就不再需要调用allocate_mm()函数了，而仅仅需要在调用copy_mm()函数中将mm域指向其父进程的内存描述符就可以了（创建线程，如果创建进程还需要上述的宏进行allocate_mm()操作）。
###15.2.2 撤销内存描述符
    当进程退出时，会调用exit_mm()函数。就是改变mm_struct中的一些和进程相关的统计，比如减少mm_users用户计数（把对内存的使用者————进程，抽象成用户）、mm_count使用计数等，为0，free_mm()宏将mm_struct结构体归还到mm_cachep slab缓存中。详询书。
###15.2.3 mm_struct与内核线程
    内核线程 没有进程地址空间，也没有相关的内存描述符。所以内核线程对应的进程描述符中mm域为空。事实上，这也正是内核线程的真实含义————它们没有用户上下文。
    下面太怪了，提几个问题，后续研究：
        内核线程并不需要访问任何用户空间的内存而且内核线程在用户空间中没有任何页，所以实际上它们并不需要有自己的内存描述符和页表。它们访问内核内存。为了避免内核线程为内存描述符和页表浪费内存，也为了当新内核线程运行时，避免浪费处理器周期向新地址空间进行切换，内核线程将直接使用前一个进程的内存描述符。
        Q1:如果前一个进程是用户进程呢？
        内核线程没有进程地址空间，也没有相关的内存描述符，进程描述符的mm域为NULL，内核发现为NULL，就会保留前一个进程的(虚拟)地址空间，随后内核更新内核线程对应的进程描述符中的active_mm域，使其指向前一个进程的内存描述符。
        Q2：我们知道，内存可装载代码段（代码的内存映射），第一反应，那它执行个毛线？但是肯定不能这样，它的代码段存哪了？
##15.3 虚拟内存区域（virtual memoryAreas，VMAs）
    内存区域由vm_area_struct结构体描述。内存区域在Linux内核中也经常称作虚拟内存区域（virtual memoryAreas，VMAs）。
###15.3.1 VMA标志（vm_area_struct. vm_flages）
    标志了内存区域所在的页面的行为和信息。和物理页的访问权限不同，VMA标志反映了内核处理页面所需要遵守的行为准则，而不是硬件要求。而且，vm_flages同时包含了内存区域中每个页面的信息，或内存区域的整体信息，而不是具体的独立页面。
        1，通过VMA标志，设置VM_READ/VM_WRITE/VM_EXEC 限制了内存区域中的访问控制权限。
        2，VM_SHARE指明了内存区域包含的映射可以再多进程间共享，如果该标志被设置，称为共享映射；未设置，则只有一个进程可使用该映射的内容，称为四有映射。
        3，VM_IO标志内存去榆中包含对设备I/O空间的映射。
        4，VM_RESERVED标志规定了内存区域不能被换出，它也是在设备驱动程序进行映射时被设置。
        5，VM_SEQ_READ标志暗示内核应用程序对映射内容（如.txt文件）执行有序的（线程和连续的）读操作；这样，内核可以有选择地执行预读操作。
        6，VM_RAND_READ标志随机读，所以内核可以选择地减少或彻底取消文件预读。
###15.3.2 VMA操作
    指定的内存区域被加入到一个地址空间、指定的内存区域从地址空间删除、当没有出现在物理内存中的页面被访问时等故障处理 等等操作。
###15.3.3 内存区域的属性结构和内存区域的链表结构
    链表用于需要遍历全部节点的时候，而红-黑树适用于在地址空间中定位特定内存区域的时候。内核为了内存区域上的各种不同操作都能获得高性能，所以同时使用了这两种数据结构。
###15.3.4 实际使用中的内存区域
    可以使用/proc（cat /proc/${pid}/maps）文件系统和pmap工具查看给定进程的内存空间和其中所含的内存区域。
    （docker使用：步骤1：docker run -d -i ${img id}  步骤2：docker exec -it ${container id} /bin/bash）
    如果一片内存范围是共享的或不可写的，那么内核只需要在内存中为文件（backing file）保留一份映射。比如C库在物理内存中仅仅需要占用1212KB空间，而不需要为每个使用C库的进程在内存中都保存一个1212KB的空间。进程访问了1340KB的数据和代码空间，然后仅仅消耗了40KB物理内存。（C库在物理内存中共享）
    注意没有映射文件的内存区域的设备标志为00:00，索引接电标志也为0，这个区域就是零页————零页映射的内容全为零。如果将零页映射到可写的内存区域，那么该区域将全被初始化为0.而bss段需要的就是全0的内存区域。由于内存未被共享，所以只要一有进程写该处数据，那么该处数据就将被拷贝出来（写到新的物理内存中）（就是我们所说的写时拷贝），然后才被更新。
    每个和进程宣贯的内存区域都对应一个vm_area_struct结构体（虚拟内存区域描述符）。另外进程不同于线程，进程描述符 task_struct包含唯一的mm_struct(内存描述符)的引用。
##15.4 操作内存区域
    内核时长需要在某个内存区域上执行一些操作，比如某个指定地址是否包含在某个内存区域中。
###15.4.1 find_vma()
    为了找到一个给定的内存地址属于哪一个内存区域，内核提供了find_vma()函数。函数返回的是vm_area_struct结构体指针。首先找mmap_cache域中的缓存，否则找红黑树。
###15.4.2 find_vma_prev()
    和find_vma()工作方式相同，但是它返回第一个小于addr的VMA。但是它返回第一个小于addr的VMA。
##15.5 mmap()和do_mmap()：创建地址区间
    内核使用do_mmap()函数创建一个新的线性地址区间。会将一个地址区间加入到进程的地址空间中————无论是扩展已存在的内存区域还是创建一个新的区域。
    如果系统调用do_mmap()的参数中有无效参数（invalid request），那么它返回一个负值；否则，它会在虚拟内存中分配一个合适的新内存区域。如果可能的话，将新区域和邻近区域进行合并，否则内核从vm_area_cachep长字节（slab）缓存中分配一个vm_area_struct结构体，并且使用vma_link()函数将新分配的内存区域添加到地址空间的内存区域链表和红黑树中，随后还要更新内存描述符中的total_vm域，然后才返回新分配的地址区间的初始地址。
    在用户空间通过mmap()系统调用获取内核函数do_mmap()的功能。
##15.6 mummap()和do_mummap()：删除地址区间
    do_mummap()函数从特定的进程地址空间中删除指定地址空间。
    系统调用mummap给用户空间程序提供了一种从自身地址空间中删除指定地址区间的方法，它和mmap()作用正好相反。
##15.7 页表
    虽然应用程序操作的对象是映射到物理内存之上的虚拟内存，但是处理器直接操作的却是物理内存。所以当用程序访问一个虚拟地址时，首先必须将虚拟地址转化成物理地址，然后处理器才能解析地址访问请求。地址的转换工作需要通过查询页表才能完成，概括地讲，地址转换需要将虚拟地址分段，使每段虚拟地址都作为一个索引指向页表，而页表项则指向下一级别的页表或者最终的物理页面。
    Linux中使用三级页表完成地址转换。利用多级页表能够节约地址转换需占用的存放空间。如果利用三级页表转换地址，即使是64位机器，占用的空间也很有限。但是如果使用静态数组实现页表，那么即便在32位机器上，该数组也将占用巨大的存放空间。
    顶级（最上级）页表是页全局目录（PGD page global directer？)，它包含了一个pgd_t类型数组，PGD中的表项指向二级页目录中的表项：PMD。
    二级页表是中间页目录（PMD），它是个pmd_t类型数组，其中的表项指向PTE中的表项。
    最后一集的页表简称页表（PTE），其中包含了pte_t类型的页表项，该页表项指向物理页面。
    多数体系结构中，搜索页表的工作是由硬件完成的。每个进程都有自己的页表（当然，线程会共享页表）。内存描述符的pgd域指向的就是进程的页全局目录。操作和检索页表时必须使用page_table_lock锁，该锁在相应的进程的内存描述符中，以防止竞争条件。
    
    由于几乎每次对虚拟内存中的页面访问都必须先解析页表（mm_struct->pgd_t->pmd_t->pte_t->页面结构->物理页面），从而得到物理内存中的对应地址，所以页表操作的性能非常关键。但不幸的是，搜索内存中的物理地址速度很有限，因此为了加快搜索，多数体系结构都实现了一个翻译后缓冲器（translate lookaside bufer，TLB）。TLB作为一个将虚拟地址映射到物理地址的硬件缓存，当请求访问一个虚拟地址时，处理器将首先检查TLB中是否缓存了该虚拟地址到物理地址的映射，如果在缓存中直接命中，物理地址立刻返回；否则，就需要再通过页表搜索需要的物理地址。
    虽然硬件完成了有关页表的部分工作，但是页表的管理仍然是内核的关键部分————而且在不断改进。2.6版内核对页表管理的主要改进是：从高端内存分配部分页表。今后可能的改进包括通过在写时拷贝（copy-on-write）的方式共享页表。这种机制使得fork()操作中可由父子进程共享页表。因为只有当子进程或父进程视图修改特定页表项时，内核才去创建该页表项的新拷贝，此后父子进程才不再共享该页表项。可以看到，利用共享页表可以消除fork（）操作中页表拷贝所带来的消耗。
##15.8
    进程虚拟内存的抽象，内核如何表示进程空间以及内核如何表示该空间中的内存区域。了解了内核如何创建和撤销内存区域，讨论了页表。因为Linux是一个机遇虚拟内存的操作系统，所以这些概念对于系统运行来说都非常基础的。
    第16章，讨论页缓存————一种用于所有页I/O操作的内存数据缓存，而且还要涵盖内核执行基于页的数据回写。
    
#第16章 页高速缓存和页回写
    页高速缓存（cache）是Linux内核实现磁盘缓存。它主要用来减少对磁盘的I/O操作。具体的讲，是通过把磁盘中的数据缓存到物理内存中，把对磁盘的访问变为对物理内存的访问。这一章将讨论页高速缓存与页回写（将页高速缓存中的变量数据刷新回磁盘的操作）。
    磁盘高速缓存之所以在任何现代操作系统中尤为重要源自两个因素：第一，访问磁盘的速度要远远低于（差好几个数量级）访问内存的速度————ms和ns的差距，因此，从内存访问数据比从磁盘访问速度更快，若从处理器的L1和L2高速缓存访问则更快。第二，数据一旦被访问，就很有可能在短期内再次被访问到。这种在短时期内集中访问同一片数据的原理称作临时局部原理（temporal locality）。临时局部原理能保证：如果在第一次访问数据时缓存它，那就极有可能在短期内再次被高速缓存命中。正是由于内存访问要比磁盘访问快得多，再加上数据一次被访问后更可能再次被访问的特点，所以磁盘的内存将给系统存储性能带来质的飞跃。
##16.1 缓存手段
    页高速缓存是由内存中的物理页面组成的，其内容对应磁盘上的物理块。页高速缓存大小能动态调整————它可以通过占用空闲内存以扩张大小，也可以自我收缩以缓解内存使用压力。我们称正被缓存的存储设备为后备存储（redis缓存mysql，mysql可称为后备存储），因为缓存背后的磁盘无疑才是所有缓存数据的归属。当内核开始一个读操作（比如，进程发起一个read()系统调用），它首先会检查需要的数据是否在页高速缓存中。如果在，则直接从内存中读取。这个行为称作缓存命中。如果数据没有在缓存中，称为缓存未命中，那么内核必须调度块I/O操作从磁盘去读取数据。然后内核将读来的数据放入页缓存中，于是任何后续相同的数据读取都可命中缓存了。注意，系统并不一定要将整个文件都缓存。缓存可以持有某个文件的全部内容，也可以存储另一些文件的一页或者几页。到底该缓存谁取决于谁被访问到。
###16.1.1 写缓存
    上面解释了读操作过程中页高速缓存的作用，那么在进程写磁盘时，比如执行write()系统调用，缓存如何被使用呢？通常来讲，缓存一般被是线程下面三种策略之一：
        1，第一种策略称为不缓存（nowrite），也就是说高速缓存不去缓存任何写操作。当堆一个缓存中的数据片进行写时，直接落盘（跳过缓存，写到磁盘），同时也使缓存中的数据失效。那么如果后续读操作进行时，需要再重新从磁盘中读取数据。不过这种策略很少使用，因为该策略不但不去缓存写操作，而且需要额外费力去使缓存数据失效。
        2，第二种策略，写操作将自动更新内存缓存，同时也更新磁盘文件。这种方式，通常称为写透缓存（write-through cache)，因为写操作会立刻穿透缓存到磁盘中。这种策略对保持缓存一致性很有好处————缓存数据时刻和后备存储保持同步，所以不需要让缓存失效，同时它的实现也最简单。
        3，第三种策略，也是Linux所采用的，称为"回写"。在这种策略下，程序执行写操作直接写到缓存中，后端存储不会立刻直接更新，而是将页高速缓存中被写入的页面标记成"脏"，并且被加入到脏页链表中。然后由一个进程（回写进程）周期的讲脏页链表中的页写回到磁盘，从而让磁盘中的数据和内存中最终一致。最后清理"脏"页标识。注意这里"脏页"这个词可能引起混淆，因为实际上脏的并非高速缓存中的数据（它们是干干净净的），而是磁盘中的数据（它们已过时了）。也许更好的描述应该是"未同步"吧。尽管如此，我们说缓存内容是脏的，而不是说磁盘内容。回写策略通常认为要好于写透策略，因为通过延迟写磁盘，方便在以后的时间内合并更多的数据和再一次刷新（只需要一次刷新多次提交并被合并的数据）。当然，其代价是实现复杂度高了许多。
###16.1.2 缓存回收
    缓存算法最后涉及的重要内容是缓存中的数据如何清除：或者是为更重要的缓存项腾出位置：或者是收缩缓存大小，腾出内存给其他地方使用。这个工作，也就是决定缓存中什么内容将被清除的策略，称为缓存回收策略。Linux的缓存回收是通过选择干净页（不脏）进行简单替换。如果缓存中没有足够的干净页面，内核将强制地进行回写操作，以腾出更多的干净可用页。最难的事情在于决定什么页应该回收。理想的回收策略应该是回收那些以后最不可能使用的页面。当然要知道以后的事情你必须是先知。也正是这个原因，理想的回收策略称为预测算法。但这种策略太理想了，无法真正实现。
    1，最近最少使用
    缓存回收策略通过所访问的数据特性，尽量追求预测效率。最成功的算法（特别是对于通用目的的页高速缓存）称作最近最少使用算法，简称LRU。LRU回收策略需要跟踪每个页面的访问踪迹（或者至少按照访问时间为序的页链表），以便能回收最老时间戳的页面（或者回收排序链表头所指的页面）。该策略的良好效果源自于缓存的数据越久未被访问，则越不大可能近期再被访问，而最近被访问的最有可能被再次访问（临时局部原理）。但是，LRU策略并非是放之四海皆准的法则，对于许多文件被访问一次，再不被访问的情景，LRU尤其失败。将这些页面放在LRU链的顶端显然不是最优，当然，内核并没办法知道一个文件只会被访问一次，但是它却知道过去访问了许多次。
    2，双链策略
    Linux实现的是一个修改过的LRU，也称为双链策略。和以前不同，Linux维护的不再是一个LRU链表，而是维护两个链表：活跃链表和非活跃链表。处于活跃链表上的页面被认为是"热"的且不会被换出，而在非活跃链表上的页面则是可以被换出的（释放物理缓存）。在活跃链表中的页面必须在其被访问时就处于非活跃链表中（？？）。两个链表都被伪LRU规则维护：页面从尾部加入，从头部移除，如同队列。两个链表需要维持平衡————如果活跃链表变得过多而超过了非活跃链表，那么活跃链表的头页面将被重新移回到非活跃链表中，以便能再被回收。双链表策略解决了传统LRU算法中对仅一次访问的窘境。而且也更加简单的实现了伪LRU的语义。这种双链表方也称作LRU/2，更普遍的是n个链表，故称LRU/n.
    我们现在知道页缓存如何固件（通过读和写），如何在写时被同步（通过回写）以及旧数据如何被回收来容纳新数据（通过双链表）。
    现在让我们看看真实世界应用场景中，页高速缓存如何帮助系统。假定你在开发一个很大的软件工程（比如Linux内核）那么你将有大量的源文件被打开，只要你打开读取源文件，这些文件就将被存储再页高速缓存中。只要数据被缓存，那么从一个文件跳到另一个文件将瞬间完成。当你编辑文件时，存储文件也会瞬间完成，因为写操作只需要写到内存，而不是磁盘。当你编译项目时，缓存的文件将使得编译过程更少访问磁盘，所以编译速度也就更快了。如果整个源码树太大了，无法一次性放入内存，那么其中一部分必须被回收————由于双链表策略，任何回收的文件都将处于非活跃链表，而且不大可能是你正在编译的文件。幸运的是，在你没在编译的时候，内核会执行页回写，刷新你所修改文件的磁盘副本。由此可见，缓存将极大地提高系统性能。为了看到差别，对比一下缓存冷（cache cold）时（也就是说重启后，编译你的大软件工程的时间）和缓存热（cache warm）时的差别吧（idea在电脑重启后打开比关闭后打开速度快）。
##16.2 Linux页高速缓存
    页高速缓存缓存的是内存页面。缓存中的页面来自对正规文件、块设备文件和内存映射文件的读写。如此一来，页高速缓存就包含了最近被访问过的文件数据块。在执行一个I/O操作前（比如read()操作），内核会检查数据是否已经在页高速缓存中了，如果所需要的数据确实在高速缓存中，那么内核可以从内存中迅速地返回需要的页，而不再需要从相对较慢的磁盘上读取数据。接下来剖析具体的数据结构以及内核如何使用它们管理缓存。
###16.2.1 address_space对象
    在页高速缓存中的页可能包含了多个不连续的物理磁盘块。正是由于页面中映射的磁盘块不一定连续，所以在页高速缓存中检查特定数据是否已经被缓存是件颇为困难的工作。因为不能用设备名称和块号来做页高速缓存中的数据的索引（不知道为啥？？？），要不然这将是最简单的定位办法。
    另外，Linux页高速缓存对被缓存的页面范围定义非常宽泛。Linux页高速缓存的目标是缓存任何基于页的对象，这包含各种类型的文件和各种类型的内存映射。
    虽然Linux页高速缓存可以通过扩展inode结构体支持页I/O操作，但这种做法会将页高速缓存局限于文件。为了维持页面高速缓存的普遍性（不应该将其绑定到物理文件或者inode结构体），Linux页高速缓存使用了一个新对象管理缓存项和页I/O操作。这个对象是address_space结构体。该结构体是第15章介绍的虚拟地址vm_area_struct的物理地址对等体。当一个文件可以被10个vm_area_struct结构体标识（比如5个进程，每个调用mmap()映射它两次），那么这个文件只能有一个address_space数据结构————也就是文件可以有多个虚拟地址，但是只能在物理内存有一份。与Linux内核中其他机构一样，address_space也是文不对题，也许更应该叫它page_cache_entity或者physical_pages_of_a_file（页映射实体，文件的物理内存页，这样的命名知道是啥了吧）。
    一个页面的读操作包含如下步骤：
        1，Linux内核试图在页高速缓存中找到需要的数据：find_get_page()方法负责完成这个检查动作。一个address_space对象和一个偏移量会传给find_get_page()方法，用于在页高速缓存中搜索需要的数据。
        2，如果搜索的页并没在高速缓存中，find_get_page()将会返回一个NULL，并且内核将分配一个新页面，然后将之前搜索的页加入到页高速缓存中。
        3，最后需要的数据从磁盘被读入，再被加入页面高速缓存，然后返回给用户。
    写操作和读操作有少许不同。对于文件映射来说，当页被修改了，VM仅仅需要调用setPageDirty(page);内核会在晚些时候通过writepage()方法把页写出。
    一个页面的写操作包含如下步骤：
        1，在页高速缓存中搜索需要的页。如果需要的页不再高速缓存中，那么内核在高速缓存中新分配一空闲项；
        2，内核创建一个写请求；
        3，数据被从用户空间拷贝到了内核缓冲；
        4，最后将数据写入磁盘。
    因为所有的页I/O操作都要执行上面这些步骤，这就保证了所有的页I/O操作必然都是通过页高速缓存进行的。因此，内核也总是试图先通过页高速缓存来满足所有的读请求。如果在页高速缓存中未搜索到需要的页，则内核将从磁盘读入需要的页，然后将该页加入到页高速缓存中；对于写操作，页高速缓存更像是一个存储平台，所有要被写出的页都要加入页高速缓存中。
###16.2.3 基树（radix tree）用以在高速缓存中找到需要的数据
    因为在任何页I/O操作前内核都要检查页是否已经在页高速缓存中了，所以这种频繁进行的检查必须迅速、高效，否则搜索和检查页高速缓存的开销可能抵消页高速缓存带来的好处（至少在缓存命中率很低的时候，搜索的开销足以抵消以内存代替磁盘进行检索数据带来的好处）。
    基树是一个二叉树，只要指定了文件偏移量，就可以在基树中迅速检索到希望的页。
###16.2.4 以前的页散列表
    在2.6版本以前，内核高速缓存不是通过基树检索，而是通过一个维护了系统中所有页的全局散列表进行检索。对于给定的一个键值，该散列表会返回一个双向链表的入口对应于这个所给定的值。
    比较散列表和基树的差别详询书
##16.3 缓冲区高速缓存
    独立的磁盘块通过块I/O缓冲也要被存入页高速缓存（注意是块I/O缓冲哦）。回忆一下第14章，一个缓冲是一个屋里磁盘块在内存里的表示。缓冲的作用就是映射内存中的页面到磁盘块，这样一来页高速缓存在块I/O操作时也减少了磁盘访问，因为它缓存磁盘块和减少块I/O操作。这个缓存通常称为缓冲区高速缓存，虽然实际上它没有座位独立缓存，而是座位页高速缓存的一部分。
    块I/O操作 一次操作一个单独的磁盘块。普遍的块I/O操作是读写i节点。内核提供了bread()函数实现从磁盘读一个块的底层操作。通过缓存，磁盘块映射到它们相关的内存页，并缓存到页高速缓存中。
    缓冲和页高速缓存并非天生就是统一的，2.4内核的主要工作之一就是统一它们（现在统一了）。在更早的内核中，有两个独立的磁盘缓存：页高速缓存和缓冲区高速缓存。前者缓存页面，后者缓存缓冲区，这两个缓存并没有统一。一个磁盘块可以同时存于两个缓存中，这导致必须同步操作两个缓冲中的数据，而且浪费了内存，去存储重复的缓存项。今天我们只有一个磁盘缓存，即页高速缓存。虽然如此，内核仍然需要在内存中使用缓冲来表示磁盘块，幸好，缓冲是用页映射块的，所以它正好在页高速缓存中。
##16.4 flusher线程
    由于页高速缓存的缓存作用，写操作实际上会被延迟。当页高速缓存中的数据比后台存储的数据更新时，该数据就称作脏数据。在内存中积累起来的脏页最终必须被写回磁盘。在以下3种情况发生时，脏页被写回磁盘：
        1，当空闲内存低于一个特定的阈值时，内核必须将脏页写回磁盘以便释放内存，因为只有干净（不脏的）内存才可以被回收。当内存干净后，内核就可以从缓存清理数据，然后收缩缓存，最终释放出更多的内存。
        2，当脏页在内存中驻留时间超过一个特定的阈值时，内核必须将超时的脏页写回磁盘，以确保脏页不会无限期地驻留在内存中。
        3，当用户进程调用sync()和fsync()系统调用时，内核会按要求执行回写动作。
    上面三种工作的目的完全不同。实际上，在旧内核中，这是由两个独立的内核线程分别完成的。但是在2.6内核中，由一群内核线程（flusher线程）执行这三种工作。
        首先，flusher线程在系统中的空闲内存低于一个特定的阈值时，将脏页刷新写回磁盘。该后台回写例程的目的在于————在可用物理内存过低时，释放脏页以重新获得内存。这个特定的内存阈值可以通过dirty_background_ratio sysctl系统调用设置。当空闲内存比阈值dirty_backgroud_ratio还低时，内核便会调用函数flusher_threads()唤醒一个或多个flusher线程，随后flusher线程进一步调用函数bdi_writeback_all()开始将脏页写回磁盘。该函数需要一个参数————试图写回的页面数目。函数连续地写出数据，直到1，已经有指定的最小数目的页被写出到磁盘。2，空闲内存数已经回收，超过了阈值dirty_backgroud_ratio。上述条件确保了flusher线程操作可以减轻系统中内存不足的压力。回写操作不会在达到这两个条件前停止，除非刷新者线程写回了所有的脏页，没有剩下的脏页可再被写回了。
        为了满足第二个目标，flusher线程后台例程会被周期性唤醒（和空闲内存是否过低无关），将那些在内存中驻留时间过长的脏页写出，确保内存中不会有长期存在的脏页。如果系统发生崩溃，由于内存处于混乱之中，所以那些在内存中还没来得及写回磁盘的脏页就会丢失，所以周期性同步页高速缓存和磁盘非常重要。在系统启动时，内核初始化一个定时器，让它周期地唤醒flusher线程，随后使其运行函数wb_writeback()。该函数将把所有驻留时间超过dirty_expire_interval ms的脏页写回。
    页回写设置的一些参数和配置看书。
###16.4.1 膝上型计算机模式
    膝上型计算机模式是一种特殊的页回写策略，该策略主要意图是将硬盘转动的机械行为最小化，允许硬盘尽可能长时间地停滞，以此延长电池供电时间。该模式可通过/proc/sys/vm/laptop_mode 文件进行配置。通常，上述配置文件内容为0，也就是说膝上型计算机模式关闭，如果需要启动膝上型计算机模式，则向配置文件中写入1。
    膝上型计算机模式的页回写行为与传统方式相比只有一处变化。处理当缓存中的页面太旧时要执行回写脏页以外，flusher还会找准磁盘运转的时机，把所有其他的物理磁盘I/O、刷新脏缓冲等通通写回到磁盘，以便保证不会专门为了写磁盘而去主动激活磁盘运行。
    上述回写行为变化要求dirty_expire_interval和dirty_writeback_interval两阈值必须设置得更大，比如10分钟。因为磁盘运转并不很频繁，所以用这样长的回写延迟就能保证膝上型计算机模式可以等到磁盘运转机会写入数据。因为关闭磁盘驱动器是节点的重要手段，膝上模式可以延长膝上计算机依靠电池的续航能力。其坏处则是系统崩溃或者其他错误会使得数据丢失。
    应用场景看书。
###16.4.2 历史上的bdflush、kupdated和pdflush（page dirty flush）
    bdflush和当前的flusher线程之间存在两个主要区别。
        1，系统中只有一个bdflush后台线程，而flusher线程的数目却是根据磁盘数量变化的；
        2，bdflush线程是基于缓冲，它将缓冲写回磁盘。相反，flusher线程基于页面，它将整个脏页写回磁盘。当然，页面可能包含缓冲，但是实际I/O操作对象是整页，而不是块。因为页在内存中是更普遍和普通的概念，所以管理页相比管理块要简单。
    因为只有在内存过低和缓冲数量过大时，bdflush例程才刷新缓冲，所以kupdated例程被引入，以便周期地写回脏页。它和pdflush线程的wb_writeback()函数提供同样的服务。
    pdflush线程的执行和今天的flusher线程类似。其主要区别在于，pdflush线程数目是动态的，默认2到8个，具体多少取决于系统I/O的负载。pdflush线程与任何任务都无关，它们是面向系统所有磁盘的全局任务。这样做的好处是实现简单，可带来的问题是，pdflush线程很容易在拥塞的磁盘上绊住，而现代硬件发生拥塞更少家常便饭。采用每个磁盘一个刷新线程可以使得I/O操作同步执行，简化了拥塞逻辑，也提升了性能。flusher线程在2.6.32内核系列中取代了pdflush线程（针对每个磁盘独立执行回写操作是其和pdflush的主要区别）。
###16.4.3 避免拥塞的方法：使用多线程
    使用bdflush线程最主要的一个缺点是，bdflush仅仅包含了一个线程，因此很有可能在页回写任务很重时，造成拥塞。这是因为单一的线程有可能堵塞在某个设备的已拥塞请求队列（正在等待将请求提交给磁盘的I/O请求队列）上，而其他设备的请求队列却没发得到处理。如何系统有多个磁盘和较强的处理能力，内核应该能使得每个磁盘都处于忙状态。不幸的是，即使还有许多数据需要回写，单个的bdflush线程也可能堵塞在某个队列的处理上，不能使所有磁盘都处于饱和的工作状态，原因在于磁盘的吞吐量是非常有限的。正是因为磁盘的吞吐量很悠闲，所以如果只有唯一线程执行页回写操作，那么这个线程很容苦苦等待对一个磁盘上的操作。为了避免出现这种情况，内核需要多个回写线程并发执行，这样单个设备队列的拥塞就不会成为系统平静了。
    2.6内核使用多个flusher线程来解决上述问题。每个线程可以相互独立地将脏页刷新回磁盘，而且不同的flusher线程处理不同的设备队列。pdflush线程策略中，线程数是动态变化的。每一个线程视图尽可能忙地从每个超级块的脏页列表中回收数据，并且写回到磁盘。pdflush方式避免了因为一个忙磁盘，而使得其余期盼饥饿的状况。但是如果每个pdflush线程在同一个拥塞的队列上挂起了又该如何呢？在这种情况下，多个pdflush线程可能并不比一个线程更好，就浪费的内存而言就要多许多。为了减轻上述影响，pdflush线程采用了拥塞避免策略：它们会主动尝试从那些没有拥塞的队列回写页。从而，pdflush线程将其工作调度开来，防止了仅仅欺负某一个忙碌设备（TCP也有拥塞避免啊）。
    方式效果确实不错，但是拥塞避免回避并不完美（这个方法还差点儿意思）。在现代操作系统中，因为I/O总线技术和计算机其他部分相比发展要缓慢得多，所以拥塞现象时常发生————处理器发展速度遵循摩尔定理，但是硬件驱动器则仅仅比20年前快一点点。flusher线程模型（又说回flusher了，开始比较了）和块设备关联，所以每个给定线程从每个给定设备的脏页链表收集数据，并写回到对应磁盘。回写于是更趋于同步了，而且由于每个磁盘对应一个线程，所以线程也不需要采用复杂的拥塞避免策略，因为一个磁盘就一个线程操作。该方法提高了I/O操作的公平性，而且降低了饥饿风险。
##16.5 小结
    Linux页高速缓存和页回写。了解内核如何通过页缓存执行页I/O操作以及这些页高速缓存（通过存储数据在内存中）可以利用减少磁盘I/O，从而极大地提升系统性能。我们套路呢通过称为“回写缓存”的进程维护在缓存中的更新页面————具体做法是标记内存中的页面为脏，然后找时机延迟写到磁盘中。

#第17章 设备与模块
    本章关于设备驱动和设备管理，我们讨论四种内核成分。
    1，设备类型：在所有Unix系统中为了统一普通设备的操作所采用的分类。
    2，模块：Linux内核中用于按需加载和卸载目标码的机制。
    3，内核对象：内核数据结构中支持面向对象的简单操作，还支持维护对象之间的父子关系。
    4，sysfs：表示系统中设备数的一个文件系统。
##17.1 设备类型
    在Linux以及所有Unix系统中，设备被分为以下三种类型：块设备、字符设备、网络设备。
    块设备通常缩写为blkdev（block device），它是可寻址的，寻址以块为单位，块大小随设备不同而不同：块设备通常支持重定位（seeking）操作，也就是对数据的随机访问。块设备有硬盘、光碟、闪存等这样的存储设备。块设备是通过称为”块设备节点“的特殊文件来访问的，并且通常被挂载为文件系统。
    字符设备通常缩写为cdev（char device），它是不可寻址的，仅提供数据的流式访问，就是一个个字符，或者一个个字节。字符设备有键盘、鼠标、打印机等。字符设备通过”字符设备节点“的特殊文件来访问。与块设备不同，应用程序通过直接访问设备节点与字符设备交互。
    网络设备最常见的类型有时也以以太网设备（ethernet devices)来称呼，它提供了对网络（例如Internet）的访问，这是通过一个物理适配器和一种特定的协议（如IP协议）进行的。网络设备到了Unix的”所有东西都是文件“的设计原则，它不是通过设备节点来访问，而是通过套接字API这样的特殊接口来访问。
    并不是所有设备驱动都表示物理设备。有些设备是虚拟的，仅提供访问内核功能而已。我们称为伪设备（pscudo device），最常见的如内核随机数发生器（通过 /dev/random 和 /dev/urandom 访问）、空设备（通过 /dev/null访问）、零设备（通过 /dev/zero 访问）、满设备（通过 /dev/full访问），还有内存设备（通过 /dev/mem访问）。然而，大部分设备驱动是表示物理设备的。
##17.2 模块
    尽管Linux是”单块内核“（monolithie）的操作系统————这是说整个系统内核都运行于一个单独的保护域中，但是Linux内核是模块化组成的，它允许内核在运行时动态地向其中插入或从中删除代码。这些代码（包括相关的子例程、数据、函数入口和函数出口）被一并组合在一个单独的二进制镜像中，即所谓的可装载内核模块中，或简称为模块。支持模块的好处是基本内核镜像可以尽可能地小，因为可选的功能和驱动程序可以利用模块形式再提供。
###17.2.1 Hello，World
    如何编写自己的内核模块
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    